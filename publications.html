<!DOCTYPE html><html lang="en" class="__className_3a0388"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width, initial-scale=1"/><link rel="preload" href="/_next/static/media/e11418ac562b8ac1-s.p.woff2" as="font" crossorigin="" type="font/woff2"/><link rel="stylesheet" href="/_next/static/css/ae2e9ad0d33ab6d9.css" data-precedence="next"/><link rel="preload" as="script" fetchPriority="low" href="/_next/static/chunks/webpack-b61c5b23e818b40e.js"/><script src="/_next/static/chunks/fd9d1056-bb11881ef41582aa.js" async=""></script><script src="/_next/static/chunks/23-d4169bb1b2b13904.js" async=""></script><script src="/_next/static/chunks/main-app-bde93d12e61991d7.js" async=""></script><script src="/_next/static/chunks/173-1b457355cc0f6eb8.js" async=""></script><script src="/_next/static/chunks/231-75217c32967b80fc.js" async=""></script><script src="/_next/static/chunks/474-3830407772278304.js" async=""></script><script src="/_next/static/chunks/app/publications/page-fe2c473f5f919a5b.js" async=""></script><script src="/_next/static/chunks/8e1d74a4-561e044f358a70a9.js" async=""></script><script src="/_next/static/chunks/521-a3cc0266cc1077a1.js" async=""></script><script src="/_next/static/chunks/443-ccb272207b22f4bd.js" async=""></script><script src="/_next/static/chunks/app/layout-e9f41310a2e3199e.js" async=""></script><script src="/_next/static/chunks/app/not-found-687fa6ca5be28a47.js" async=""></script><title>LARA</title><meta name="description" content="The Laboratory for AI, Robotics, and Automation (LARA) at UC Davis is at the forefront of research in artificial intelligence and robotics, focusing on the development of autonomous systems that can learn, adapt, and operate in complex environments. Our interdisciplinary research spans across cutting-edge technologies, including machine learning, reinforcement learning, and robotic systems, with applications in automation, autonomous vehicles, and human-robot interaction."/><meta name="robots" content="index, follow"/><meta name="googlebot" content="index, follow, max-video-preview:-1, max-image-preview:large, max-snippet:-1"/><meta property="og:title" content="LARA"/><meta property="og:description" content="The Laboratory for AI, Robotics, and Automation (LARA) at UC Davis is at the forefront of research in artificial intelligence and robotics, focusing on the development of autonomous systems that can learn, adapt, and operate in complex environments. Our interdisciplinary research spans across cutting-edge technologies, including machine learning, reinforcement learning, and robotic systems, with applications in automation, autonomous vehicles, and human-robot interaction."/><meta property="og:url" content="https://dillion.io"/><meta property="og:site_name" content="LARA"/><meta property="og:locale" content="en_US"/><meta property="og:type" content="website"/><meta name="twitter:card" content="summary_large_image"/><meta name="twitter:title" content="LARA"/><meta name="twitter:description" content="The Laboratory for AI, Robotics, and Automation (LARA) at UC Davis is at the forefront of research in artificial intelligence and robotics, focusing on the development of autonomous systems that can learn, adapt, and operate in complex environments. Our interdisciplinary research spans across cutting-edge technologies, including machine learning, reinforcement learning, and robotic systems, with applications in automation, autonomous vehicles, and human-robot interaction."/><link rel="icon" href="/favicon.ico" type="image/x-icon" sizes="16x16"/><meta name="next-size-adjust"/><script src="/_next/static/chunks/polyfills-78c92fac7aa8fdd8.js" noModule=""></script></head><body class="antialiased bg-gray-50"><script>!function(){try{var d=document.documentElement,c=d.classList;c.remove('light','dark');var e=localStorage.getItem('theme');if('system'===e||(!e&&false)){var t='(prefers-color-scheme: dark)',m=window.matchMedia(t);if(m.media!==t||m.matches){d.style.colorScheme = 'dark';c.add('dark')}else{d.style.colorScheme = 'light';c.add('light')}}else if(e){c.add(e|| '')}else{c.add('light')}if(e==='light'||e==='dark'||!e)d.style.colorScheme=e||'light'}catch(e){}}()</script><div class="min-h-screen p-3"><div class="fixed inset-3 flex rounded-xl bg-white shadow-sm"><button class="fixed top-4 right-4 z-50 p-2 rounded-md bg-white shadow-sm sm:hidden"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-menu"><line x1="4" x2="20" y1="12" y2="12"></line><line x1="4" x2="20" y1="6" y2="6"></line><line x1="4" x2="20" y1="18" y2="18"></line></svg></button><aside class="fixed inset-y-0 left-0 z-40 w-64 bg-white transform transition-transform duration-200 ease-in-out sm:relative sm:transform-none sm:w-44 sm:border-r sm:border-gray-100 -translate-x-full sm:translate-x-0"><div class="sticky top-0 flex h-full flex-col justify-between py-2"><div><a class="flex items-center justify-center px-5 pt-8 pb-12" href="/"><img alt="LARA Lab" loading="lazy" width="140" height="140" decoding="async" data-nimg="1" style="color:transparent" srcSet="/_next/image?url=%2FLARALOGO.png&amp;w=256&amp;q=75 1x, /_next/image?url=%2FLARALOGO.png&amp;w=384&amp;q=75 2x" src="/_next/image?url=%2FLARALOGO.png&amp;w=384&amp;q=75"/></a><nav class="space-y-1 px-2 mb-8"><a class="flex items-center px-3 py-2 text-[13px] transition-all duration-200 rounded-md hover:scale-105 hover:bg-gray-50 text-gray-500 hover:text-gray-900" href="/">Home</a><a class="flex items-center px-3 py-2 text-[13px] transition-all duration-200 rounded-md hover:scale-105 hover:bg-gray-50 text-gray-500 hover:text-gray-900" href="/about">About</a><a class="flex items-center px-3 py-2 text-[13px] transition-all duration-200 rounded-md hover:scale-105 hover:bg-gray-50 text-black font-medium bg-gray-50" href="/publications">Publications</a><a class="flex items-center px-3 py-2 text-[13px] transition-all duration-200 rounded-md hover:scale-105 hover:bg-gray-50 text-gray-500 hover:text-gray-900" href="/patents">Patents</a><a class="flex items-center px-3 py-2 text-[13px] transition-all duration-200 rounded-md hover:scale-105 hover:bg-gray-50 text-gray-500 hover:text-gray-900" href="/blog">News</a><a class="flex items-center px-3 py-2 text-[13px] transition-all duration-200 rounded-md hover:scale-105 hover:bg-gray-50 text-gray-500 hover:text-gray-900" href="/people">People</a><a class="flex items-center px-3 py-2 text-[13px] transition-all duration-200 rounded-md hover:scale-105 hover:bg-gray-50 text-gray-500 hover:text-gray-900" href="/faq">FAQ</a><a class="flex items-center px-3 py-2 text-[13px] transition-all duration-200 rounded-md hover:scale-105 hover:bg-gray-50 text-gray-500 hover:text-gray-900" href="/contact">Contact</a></nav></div><div class="space-y-8 px-5 pb-6"><a href="https://drive.google.com/file/d/1PuFn_1D4t18b7H_9VPRS3ilNOY-dlGjO/view" target="_blank" rel="noopener noreferrer" class="group flex items-center gap-2 text-[11px] text-gray-400 hover:text-gray-900 transition-colors"><span>View brand guidelines</span><svg xmlns="http://www.w3.org/2000/svg" width="10" height="10" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-arrow-right transition-transform group-hover:translate-x-0.5"><path d="M5 12h14"></path><path d="m12 5 7 7-7 7"></path></svg></a><div class="flex gap-4"><a href="https://linkedin.com" target="_blank" rel="noopener noreferrer" class="text-gray-400 hover:text-gray-900 transition-all hover:scale-110 duration-200" aria-label="LinkedIn"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 448 512" height="15" width="15" xmlns="http://www.w3.org/2000/svg"><path d="M416 32H31.9C14.3 32 0 46.5 0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6 0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3zM135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3 0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2 0 38.5 17.3 38.5 38.5 0 21.3-17.2 38.5-38.5 38.5zm282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6 0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2 0 79.7 44.3 79.7 101.9V416z"></path></svg></a><a href="https://scholar.google.com" target="_blank" rel="noopener noreferrer" class="text-gray-400 hover:text-gray-900 transition-all hover:scale-110 duration-200" aria-label="Google Scholar"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 488 512" height="15" width="15" xmlns="http://www.w3.org/2000/svg"><path d="M488 261.8C488 403.3 391.1 504 248 504 110.8 504 0 393.2 0 256S110.8 8 248 8c66.8 0 123 24.5 166.3 64.9l-67.5 64.9C258.5 52.6 94.3 116.6 94.3 256c0 86.5 69.1 156.6 153.7 156.6 98.2 0 135-70.4 140.8-106.9H248v-85.3h236.1c2.3 12.7 3.9 24.9 3.9 41.4z"></path></svg></a><a href="https://github.com" target="_blank" rel="noopener noreferrer" class="text-gray-400 hover:text-gray-900 transition-all hover:scale-110 duration-200" aria-label="GitHub"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 496 512" height="15" width="15" xmlns="http://www.w3.org/2000/svg"><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"></path></svg></a><a href="mailto:isoltani@ucdavis.edu" class="text-gray-400 hover:text-gray-900 transition-all hover:scale-110 duration-200" aria-label="Email"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 512 512" height="15" width="15" xmlns="http://www.w3.org/2000/svg"><path d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"></path></svg></a></div></div></div></aside><main class="flex-1 overflow-auto"><div class="w-full"><main class="flex flex-col"><section class="pt-20 pb-16 px-6 md:px-10"><div class="max-w-5xl mx-auto"><p class="text-sm text-gray-600 mb-4">Research</p><div class="flex flex-col gap-2"><h1 class="text-[64px] font-medium tracking-tight leading-none mb-2">Publications</h1></div></div></section><div class="px-6 md:px-10"><div class="max-w-5xl mx-auto"><div class="flex gap-4 mb-8"><button class="flex items-center gap-1 text-sm text-gray-600 hover:text-black transition-colors">Sort by Year<svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-chevron-down"><path d="m6 9 6 6 6-6"></path></svg></button></div><div class="space-y-12"><div class="flex flex-col md:flex-row gap-8 border-b border-gray-100 pb-12 last:border-0"><div class="md:w-1/3 aspect-video relative overflow-hidden rounded-lg bg-gray-100"><img alt="Automating Infrastructure Surveying: A Framework for Geometric Measurements and Compliance Assessment Using Point Cloud Data" loading="lazy" decoding="async" data-nimg="fill" class="object-cover" style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;color:transparent" sizes="100vw" srcSet="/_next/image?url=%2Fpublications%2FAutomating.png&amp;w=640&amp;q=75 640w, /_next/image?url=%2Fpublications%2FAutomating.png&amp;w=750&amp;q=75 750w, /_next/image?url=%2Fpublications%2FAutomating.png&amp;w=828&amp;q=75 828w, /_next/image?url=%2Fpublications%2FAutomating.png&amp;w=1080&amp;q=75 1080w, /_next/image?url=%2Fpublications%2FAutomating.png&amp;w=1200&amp;q=75 1200w, /_next/image?url=%2Fpublications%2FAutomating.png&amp;w=1920&amp;q=75 1920w, /_next/image?url=%2Fpublications%2FAutomating.png&amp;w=2048&amp;q=75 2048w, /_next/image?url=%2Fpublications%2FAutomating.png&amp;w=3840&amp;q=75 3840w" src="/_next/image?url=%2Fpublications%2FAutomating.png&amp;w=3840&amp;q=75"/></div><div class="md:w-2/3"><a target="_blank" rel="noopener noreferrer" class="group" href="https://arxiv.org/abs/2505.05752"><h3 class="text-lg font-medium mb-3 group-hover:text-blue-600 transition-colors">Automating Infrastructure Surveying: A Framework for Geometric Measurements and Compliance Assessment Using Point Cloud Data</h3></a><div class="flex flex-wrap gap-2 mb-3 text-sm text-gray-600"><span>Amin Ghafourian<!-- -->,</span><span>Andrew Lee<!-- -->,</span><span>Dechen Gao<!-- -->,</span><span>Tyler Beer<!-- -->,</span><span>Kin Yen<!-- -->,</span><span>Iman Soltani</span></div><div class="flex gap-4 text-sm"><span class="text-gray-600">arXiv preprint</span><span class="text-gray-500 capitalize">preprint</span><span class="text-gray-600">2025</span></div></div></div><div class="flex flex-col md:flex-row gap-8 border-b border-gray-100 pb-12 last:border-0"><div class="md:w-1/3 aspect-video relative overflow-hidden rounded-lg bg-gray-100"><video class="w-full h-full object-cover" poster="/publications/krysalis.mp4" autoPlay="" muted="" loop="" playsInline="" preload="auto"><source src="/publications/krysalis.mp4" type="video/mp4"/></video></div><div class="md:w-2/3"><a target="_blank" rel="noopener noreferrer" class="group" href="https://arxiv.org/abs/2504.12967"><h3 class="text-lg font-medium mb-3 group-hover:text-blue-600 transition-colors">Krysalis Hand: A Lightweight, High-Payload, 18-DoF Anthropomorphic End-Effector for Robotic Learning and Dexterous Manipulation</h3></a><div class="flex flex-wrap gap-2 mb-3 text-sm text-gray-600"><span>A Basheer<!-- -->,</span><span>J Chang<!-- -->,</span><span>Y Chen<!-- -->,</span><span>D Kim<!-- -->,</span><span>I Soltani</span></div><div class="flex gap-4 text-sm"><span class="text-gray-600">arXiv preprint</span><span class="text-gray-500 capitalize">preprint</span><span class="text-gray-600">2025</span></div></div></div><div class="flex flex-col md:flex-row gap-8 border-b border-gray-100 pb-12 last:border-0"><div class="md:w-1/3 aspect-video relative overflow-hidden rounded-lg bg-gray-100"><img alt="User-Centered Insights into Assistive Navigation Technologies for Individuals with Visual Impairment" loading="lazy" decoding="async" data-nimg="fill" class="object-cover" style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;color:transparent" sizes="100vw" srcSet="/_next/image?url=%2Fpublications%2Fassistive.png&amp;w=640&amp;q=75 640w, /_next/image?url=%2Fpublications%2Fassistive.png&amp;w=750&amp;q=75 750w, /_next/image?url=%2Fpublications%2Fassistive.png&amp;w=828&amp;q=75 828w, /_next/image?url=%2Fpublications%2Fassistive.png&amp;w=1080&amp;q=75 1080w, /_next/image?url=%2Fpublications%2Fassistive.png&amp;w=1200&amp;q=75 1200w, /_next/image?url=%2Fpublications%2Fassistive.png&amp;w=1920&amp;q=75 1920w, /_next/image?url=%2Fpublications%2Fassistive.png&amp;w=2048&amp;q=75 2048w, /_next/image?url=%2Fpublications%2Fassistive.png&amp;w=3840&amp;q=75 3840w" src="/_next/image?url=%2Fpublications%2Fassistive.png&amp;w=3840&amp;q=75"/></div><div class="md:w-2/3"><a target="_blank" rel="noopener noreferrer" class="group" href="https://arxiv.org/abs/2504.06379"><h3 class="text-lg font-medium mb-3 group-hover:text-blue-600 transition-colors">User-Centered Insights into Assistive Navigation Technologies for Individuals with Visual Impairment</h3></a><div class="flex flex-wrap gap-2 mb-3 text-sm text-gray-600"><span>I Soltani<!-- -->,</span><span>J Schofield<!-- -->,</span><span>M Madani<!-- -->,</span><span>D Kish<!-- -->,</span><span>P Emami-Naeini</span></div><div class="flex gap-4 text-sm"><span class="text-gray-600">arXiv preprint</span><span class="text-gray-500 capitalize">preprint</span><span class="text-gray-600">2025</span></div></div></div><div class="flex flex-col md:flex-row gap-8 border-b border-gray-100 pb-12 last:border-0"><div class="md:w-1/3 aspect-video relative overflow-hidden rounded-lg bg-gray-100"><video class="w-full h-full object-cover" poster="/publications/harsh1.mp4" autoPlay="" muted="" loop="" playsInline="" preload="auto"><source src="/publications/harsh1.mp4" type="video/mp4"/></video></div><div class="md:w-2/3"><a target="_blank" rel="noopener noreferrer" class="group" href="https://arxiv.org/abs/2504.14078"><h3 class="text-lg font-medium mb-3 group-hover:text-blue-600 transition-colors">Infrared Vision Systems for Emergency Vehicle Driver Assistance in Low-Visibility Conditions</h3></a><div class="flex flex-wrap gap-2 mb-3 text-sm text-gray-600"><span>MM Naddaf-Sh<!-- -->,</span><span>A Lee<!-- -->,</span><span>K Yen<!-- -->,</span><span>E Amini<!-- -->,</span><span>I Soltani</span></div><div class="flex gap-4 text-sm"><span class="text-gray-600">arXiv preprint</span><span class="text-gray-500 capitalize">preprint</span><span class="text-gray-600">2025</span></div></div></div><div class="flex flex-col md:flex-row gap-8 border-b border-gray-100 pb-12 last:border-0"><div class="md:w-1/3 aspect-video relative overflow-hidden rounded-lg bg-gray-100"><video class="w-full h-full object-cover" poster="/publications/dual.mp4" autoPlay="" muted="" loop="" playsInline="" preload="auto"><source src="/publications/dual.mp4" type="video/mp4"/></video></div><div class="md:w-2/3"><a target="_blank" rel="noopener noreferrer" class="group" href="https://arxiv.org/abs/2502.12539"><h3 class="text-lg font-medium mb-3 group-hover:text-blue-600 transition-colors">Design and Implementation of a Dual Uncrewed Surface Vessel Platform for Bathymetry Research under High-flow Conditions</h3></a><div class="flex flex-wrap gap-2 mb-3 text-sm text-gray-600"><span>D Kumar<!-- -->,</span><span>A Ghorbanpour<!-- -->,</span><span>K Yen<!-- -->,</span><span>I Soltani</span></div><div class="flex gap-4 text-sm"><span class="text-gray-600">arXiv</span><span class="text-gray-500 capitalize">preprint</span><span class="text-gray-600">2025</span></div></div></div><div class="flex flex-col md:flex-row gap-8 border-b border-gray-100 pb-12 last:border-0"><div class="md:w-1/3 aspect-video relative overflow-hidden rounded-lg bg-gray-100"><img alt="Active vision might be all you need: Exploring active vision in bimanual robotic manipulation" loading="lazy" decoding="async" data-nimg="fill" class="object-cover" style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;color:transparent" sizes="100vw" srcSet="/_next/image?url=%2Fpublications%2Fav-aloha.gif&amp;w=640&amp;q=75 640w, /_next/image?url=%2Fpublications%2Fav-aloha.gif&amp;w=750&amp;q=75 750w, /_next/image?url=%2Fpublications%2Fav-aloha.gif&amp;w=828&amp;q=75 828w, /_next/image?url=%2Fpublications%2Fav-aloha.gif&amp;w=1080&amp;q=75 1080w, /_next/image?url=%2Fpublications%2Fav-aloha.gif&amp;w=1200&amp;q=75 1200w, /_next/image?url=%2Fpublications%2Fav-aloha.gif&amp;w=1920&amp;q=75 1920w, /_next/image?url=%2Fpublications%2Fav-aloha.gif&amp;w=2048&amp;q=75 2048w, /_next/image?url=%2Fpublications%2Fav-aloha.gif&amp;w=3840&amp;q=75 3840w" src="/_next/image?url=%2Fpublications%2Fav-aloha.gif&amp;w=3840&amp;q=75"/></div><div class="md:w-2/3"><a target="_blank" rel="noopener noreferrer" class="group" href="https://arxiv.org/abs/2409.17435"><h3 class="text-lg font-medium mb-3 group-hover:text-blue-600 transition-colors">Active vision might be all you need: Exploring active vision in bimanual robotic manipulation</h3></a><div class="flex flex-wrap gap-2 mb-3 text-sm text-gray-600"><span>I Chuang<!-- -->,</span><span>A Lee<!-- -->,</span><span>D Gao<!-- -->,</span><span>I Soltani</span></div><div class="flex gap-4 text-sm"><span class="text-gray-600">IEEE International Conference on Robotics and Automation (ICRA)</span><span class="text-gray-500 capitalize">conference</span><span class="text-gray-600">2025</span></div></div></div><div class="flex flex-col md:flex-row gap-8 border-b border-gray-100 pb-12 last:border-0"><div class="md:w-1/3 aspect-video relative overflow-hidden rounded-lg bg-gray-100"><img alt="Targeted collapse regularized autoencoder for anomaly detection: black hole at the center" loading="lazy" decoding="async" data-nimg="fill" class="object-cover" style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;color:transparent" sizes="100vw" srcSet="/_next/image?url=%2Fpublications%2Fcollapse.png&amp;w=640&amp;q=75 640w, /_next/image?url=%2Fpublications%2Fcollapse.png&amp;w=750&amp;q=75 750w, /_next/image?url=%2Fpublications%2Fcollapse.png&amp;w=828&amp;q=75 828w, /_next/image?url=%2Fpublications%2Fcollapse.png&amp;w=1080&amp;q=75 1080w, /_next/image?url=%2Fpublications%2Fcollapse.png&amp;w=1200&amp;q=75 1200w, /_next/image?url=%2Fpublications%2Fcollapse.png&amp;w=1920&amp;q=75 1920w, /_next/image?url=%2Fpublications%2Fcollapse.png&amp;w=2048&amp;q=75 2048w, /_next/image?url=%2Fpublications%2Fcollapse.png&amp;w=3840&amp;q=75 3840w" src="/_next/image?url=%2Fpublications%2Fcollapse.png&amp;w=3840&amp;q=75"/></div><div class="md:w-2/3"><a target="_blank" rel="noopener noreferrer" class="group" href="https://ieeexplore.ieee.org/abstract/document/10720395"><h3 class="text-lg font-medium mb-3 group-hover:text-blue-600 transition-colors">Targeted collapse regularized autoencoder for anomaly detection: black hole at the center</h3></a><div class="flex flex-wrap gap-2 mb-3 text-sm text-gray-600"><span>A Ghafourian<!-- -->,</span><span>H Shui<!-- -->,</span><span>D Upadhyay<!-- -->,</span><span>R Gupta<!-- -->,</span><span>D Filev<!-- -->,</span><span>I Soltani</span></div><div class="flex gap-4 text-sm"><span class="text-gray-600">IEEE Transactions on Neural Networks and Learning Systems</span><span class="text-gray-500 capitalize">journal</span><span class="text-gray-600">2024</span></div></div></div><div class="flex flex-col md:flex-row gap-8 border-b border-gray-100 pb-12 last:border-0"><div class="md:w-1/3 aspect-video relative overflow-hidden rounded-lg bg-gray-100"><video class="w-full h-full object-cover" poster="/publications/cardreamer.gif" autoPlay="" muted="" loop="" playsInline="" preload="auto"><source src="/publications/cardreamer.gif" type="video/mp4"/></video></div><div class="md:w-2/3"><a target="_blank" rel="noopener noreferrer" class="group" href="https://ieeexplore.ieee.org/abstract/document/10714437"><h3 class="text-lg font-medium mb-3 group-hover:text-blue-600 transition-colors">Cardreamer: Open-source learning platform for world model based autonomous driving</h3></a><div class="flex flex-wrap gap-2 mb-3 text-sm text-gray-600"><span>D Gao<!-- -->,</span><span>S Cai<!-- -->,</span><span>H Zhou<!-- -->,</span><span>H Wang<!-- -->,</span><span>I Soltani<!-- -->,</span><span>J Zhang</span></div><div class="flex gap-4 text-sm"><span class="text-gray-600">IEEE Internet of Things Journal</span><span class="text-gray-500 capitalize">journal</span><span class="text-gray-600">2024</span></div></div></div><div class="flex flex-col md:flex-row gap-8 border-b border-gray-100 pb-12 last:border-0"><div class="md:w-1/3 aspect-video relative overflow-hidden rounded-lg bg-gray-100"><video class="w-full h-full object-cover" poster="/publications/USV_MarineFormer.mp4" autoPlay="" muted="" loop="" playsInline="" preload="auto"><source src="/publications/USV_MarineFormer.mp4" type="video/mp4"/></video></div><div class="md:w-2/3"><a target="_blank" rel="noopener noreferrer" class="group" href="https://arxiv.org/abs/2410.13973"><h3 class="text-lg font-medium mb-3 group-hover:text-blue-600 transition-colors">MarineFormer: A Transformer-based Navigation Policy Model for Collision Avoidance in Marine Environment</h3></a><div class="flex flex-wrap gap-2 mb-3 text-sm text-gray-600"><span>E Kazemi<!-- -->,</span><span>I Soltani</span></div><div class="flex gap-4 text-sm"><span class="text-gray-600">arXiv e-prints</span><span class="text-gray-500 capitalize">preprint</span><span class="text-gray-600">2024</span></div></div></div><div class="flex flex-col md:flex-row gap-8 border-b border-gray-100 pb-12 last:border-0"><div class="md:w-1/3 aspect-video relative overflow-hidden rounded-lg bg-gray-100"><video class="w-full h-full object-cover" poster="/publications/InteractDemo.mp4" autoPlay="" muted="" loop="" playsInline="" preload="auto"><source src="/publications/InteractDemo.mp4" type="video/mp4"/></video></div><div class="md:w-2/3"><a target="_blank" rel="noopener noreferrer" class="group" href="https://arxiv.org/abs/2409.07914"><h3 class="text-lg font-medium mb-3 group-hover:text-blue-600 transition-colors">InterACT: Inter-dependency Aware Action Chunking with Hierarchical Attention Transformers for Bimanual Manipulation</h3></a><div class="flex flex-wrap gap-2 mb-3 text-sm text-gray-600"><span>A Lee<!-- -->,</span><span>I Chuang<!-- -->,</span><span>LY Chen<!-- -->,</span><span>I Soltani</span></div><div class="flex gap-4 text-sm"><span class="text-gray-600">Conference on Robot Learning (CoRL)</span><span class="text-gray-500 capitalize">conference</span><span class="text-gray-600">2024</span></div></div></div><div class="flex flex-col md:flex-row gap-8 border-b border-gray-100 pb-12 last:border-0"><div class="md:w-1/3 aspect-video relative overflow-hidden rounded-lg bg-gray-100"><img alt="Performance of Automated Machine Learning in Predicting Outcomes of Pneumatic Retinopexy" loading="lazy" decoding="async" data-nimg="fill" class="object-cover" style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;color:transparent" sizes="100vw" srcSet="/_next/image?url=%2Fpublications%2Fretino.png&amp;w=640&amp;q=75 640w, /_next/image?url=%2Fpublications%2Fretino.png&amp;w=750&amp;q=75 750w, /_next/image?url=%2Fpublications%2Fretino.png&amp;w=828&amp;q=75 828w, /_next/image?url=%2Fpublications%2Fretino.png&amp;w=1080&amp;q=75 1080w, /_next/image?url=%2Fpublications%2Fretino.png&amp;w=1200&amp;q=75 1200w, /_next/image?url=%2Fpublications%2Fretino.png&amp;w=1920&amp;q=75 1920w, /_next/image?url=%2Fpublications%2Fretino.png&amp;w=2048&amp;q=75 2048w, /_next/image?url=%2Fpublications%2Fretino.png&amp;w=3840&amp;q=75 3840w" src="/_next/image?url=%2Fpublications%2Fretino.png&amp;w=3840&amp;q=75"/></div><div class="md:w-2/3"><a target="_blank" rel="noopener noreferrer" class="group" href="https://www.sciencedirect.com/science/article/pii/S266691452400006X"><h3 class="text-lg font-medium mb-3 group-hover:text-blue-600 transition-colors">Performance of Automated Machine Learning in Predicting Outcomes of Pneumatic Retinopexy</h3></a><div class="flex flex-wrap gap-2 mb-3 text-sm text-gray-600"><span>A Nisanova<!-- -->,</span><span>A Yavary<!-- -->,</span><span>J Deaner<!-- -->,</span><span>FS Ali<!-- -->,</span><span>P Gogte<!-- -->,</span><span>R Kaplan<!-- -->,</span><span>KC Chen</span></div><div class="flex gap-4 text-sm"><span class="text-gray-600">Ophthalmology Science 4 (5), 100470</span><span class="text-gray-500 capitalize">journal</span><span class="text-gray-600">2024</span></div></div></div><div class="flex flex-col md:flex-row gap-8 border-b border-gray-100 pb-12 last:border-0"><div class="md:w-1/3 aspect-video relative overflow-hidden rounded-lg bg-gray-100"><video class="w-full h-full object-cover" poster="/publications/AV_Fully_Compliant_Lane_Change_Compliance_Ablation_movie.mp4" autoPlay="" muted="" loop="" playsInline="" preload="auto"><source src="/publications/AV_Fully_Compliant_Lane_Change_Compliance_Ablation_movie.mp4" type="video/mp4"/></video></div><div class="md:w-2/3"><a target="_blank" rel="noopener noreferrer" class="group" href="https://www.sciencedirect.com/science/article/pii/S2590198224001994"><h3 class="text-lg font-medium mb-3 group-hover:text-blue-600 transition-colors">Assessing the impact of driver compliance on traffic flow and safety in work zones amid varied mixed autonomy scenarios</h3></a><div class="flex flex-wrap gap-2 mb-3 text-sm text-gray-600"><span>IS Ehsan Kazemi</span></div><div class="flex gap-4 text-sm"><span class="text-gray-600">Transportation Research Interdisciplinary Perspectives 27</span><span class="text-gray-500 capitalize">journal</span><span class="text-gray-600">2024</span></div></div></div><div class="flex flex-col md:flex-row gap-8 border-b border-gray-100 pb-12 last:border-0"><div class="md:w-1/3 aspect-video relative overflow-hidden rounded-lg bg-gray-100"><video class="w-full h-full object-cover" autoPlay="" muted="" loop="" playsInline="" preload="auto"><source src="/publications/demo_autodrive.mp4" type="video/mp4"/></video></div><div class="md:w-2/3"><a target="_blank" rel="noopener noreferrer" class="group" href="https://ieeexplore.ieee.org/abstract/document/10433735"><h3 class="text-lg font-medium mb-3 group-hover:text-blue-600 transition-colors">Hierarchical end-to-end autonomous navigation through few-shot waypoint detection</h3></a><div class="flex flex-wrap gap-2 mb-3 text-sm text-gray-600"><span>A Ghafourian<!-- -->,</span><span>Z CuiZhu<!-- -->,</span><span>D Shi<!-- -->,</span><span>I Chuang<!-- -->,</span><span>F Charette<!-- -->,</span><span>R Sachdeva</span></div><div class="flex gap-4 text-sm"><span class="text-gray-600">IEEE Robotics and Automation Letters 9 (4), 3211-3218</span><span class="text-gray-500 capitalize">journal</span><span class="text-gray-600">2024</span></div></div></div><div class="flex flex-col md:flex-row gap-8 border-b border-gray-100 pb-12 last:border-0"><div class="md:w-1/3 aspect-video relative overflow-hidden rounded-lg bg-gray-100"><video class="w-full h-full object-cover" poster="/publications/deepbayesian.mp4" autoPlay="" muted="" loop="" playsInline="" preload="auto"><source src="/publications/deepbayesian.mp4" type="video/mp4"/></video></div><div class="md:w-2/3"><a target="_blank" rel="noopener noreferrer" class="group" href="https://www.mdpi.com/1424-8220/23/13/6107"><h3 class="text-lg font-medium mb-3 group-hover:text-blue-600 transition-colors">Deep Bayesian-Assisted Keypoint Detection for Pose Estimation in Assembly Automation</h3></a><div class="flex flex-wrap gap-2 mb-3 text-sm text-gray-600"><span>IS Debo Shi<!-- -->,</span><span>Alireza Rahimpour<!-- -->,</span><span>Amin Ghafourian<!-- -->,</span><span>Mohammad Mahdi Naddaf</span></div><div class="flex gap-4 text-sm"><span class="text-gray-600">Sensors 23 (13)</span><span class="text-gray-500 capitalize">journal</span><span class="text-gray-600">2023</span></div></div></div><div class="flex flex-col md:flex-row gap-8 border-b border-gray-100 pb-12 last:border-0"><div class="md:w-1/3 aspect-video relative overflow-hidden rounded-lg bg-gray-100"><video class="w-full h-full object-cover" poster="/publications/videoplayback.mp4" autoPlay="" muted="" loop="" playsInline="" preload="auto"><source src="/publications/videoplayback.mp4" type="video/mp4"/></video></div><div class="md:w-2/3"><a target="_blank" rel="noopener noreferrer" class="group" href="https://arxiv.org/abs/2306.08865"><h3 class="text-lg font-medium mb-3 group-hover:text-blue-600 transition-colors">One-Shot Learning of Visual Path Navigation for Autonomous Vehicles</h3></a><div class="flex flex-wrap gap-2 mb-3 text-sm text-gray-600"><span>Z CuiZhu<!-- -->,</span><span>F Charette<!-- -->,</span><span>A Ghafourian<!-- -->,</span><span>D Shi<!-- -->,</span><span>M Cui<!-- -->,</span><span>A Krishnamachar</span></div><div class="flex gap-4 text-sm"><span class="text-gray-600">Workshop on Machine Learning for Autonomous Driving (ML4AD)</span><span class="text-gray-500 capitalize">conference</span><span class="text-gray-600">2023</span></div></div></div><div class="flex flex-col md:flex-row gap-8 border-b border-gray-100 pb-12 last:border-0"><div class="md:w-1/3 aspect-video relative overflow-hidden rounded-lg bg-gray-100"><img alt="Anomaly detection with domain adaptation" loading="lazy" decoding="async" data-nimg="fill" class="object-cover" style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;color:transparent" sizes="100vw" srcSet="/_next/image?url=%2Fpublications%2FAnomalydetection.png&amp;w=640&amp;q=75 640w, /_next/image?url=%2Fpublications%2FAnomalydetection.png&amp;w=750&amp;q=75 750w, /_next/image?url=%2Fpublications%2FAnomalydetection.png&amp;w=828&amp;q=75 828w, /_next/image?url=%2Fpublications%2FAnomalydetection.png&amp;w=1080&amp;q=75 1080w, /_next/image?url=%2Fpublications%2FAnomalydetection.png&amp;w=1200&amp;q=75 1200w, /_next/image?url=%2Fpublications%2FAnomalydetection.png&amp;w=1920&amp;q=75 1920w, /_next/image?url=%2Fpublications%2FAnomalydetection.png&amp;w=2048&amp;q=75 2048w, /_next/image?url=%2Fpublications%2FAnomalydetection.png&amp;w=3840&amp;q=75 3840w" src="/_next/image?url=%2Fpublications%2FAnomalydetection.png&amp;w=3840&amp;q=75"/></div><div class="md:w-2/3"><a target="_blank" rel="noopener noreferrer" class="group" href="https://openaccess.thecvf.com/content/CVPR2023W/VAND/html/Yang_Anomaly_Detection_With_Domain_Adaptation_CVPRW_2023_paper.html"><h3 class="text-lg font-medium mb-3 group-hover:text-blue-600 transition-colors">Anomaly detection with domain adaptation</h3></a><div class="flex flex-wrap gap-2 mb-3 text-sm text-gray-600"><span>Z Yang<!-- -->,</span><span>I Soltani<!-- -->,</span><span>E Darve</span></div><div class="flex gap-4 text-sm"><span class="text-gray-600">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</span><span class="text-gray-500 capitalize">conference</span><span class="text-gray-600">2023</span></div></div></div><div class="flex flex-col md:flex-row gap-8 border-b border-gray-100 pb-12 last:border-0"><div class="md:w-1/3 aspect-video relative overflow-hidden rounded-lg bg-gray-100"><img alt="Memory-augmented generative adversarial networks for anomaly detection" loading="lazy" decoding="async" data-nimg="fill" class="object-cover" style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;color:transparent" sizes="100vw" srcSet="/_next/image?url=%2Fpublications%2FMemory-augmented.png&amp;w=640&amp;q=75 640w, /_next/image?url=%2Fpublications%2FMemory-augmented.png&amp;w=750&amp;q=75 750w, /_next/image?url=%2Fpublications%2FMemory-augmented.png&amp;w=828&amp;q=75 828w, /_next/image?url=%2Fpublications%2FMemory-augmented.png&amp;w=1080&amp;q=75 1080w, /_next/image?url=%2Fpublications%2FMemory-augmented.png&amp;w=1200&amp;q=75 1200w, /_next/image?url=%2Fpublications%2FMemory-augmented.png&amp;w=1920&amp;q=75 1920w, /_next/image?url=%2Fpublications%2FMemory-augmented.png&amp;w=2048&amp;q=75 2048w, /_next/image?url=%2Fpublications%2FMemory-augmented.png&amp;w=3840&amp;q=75 3840w" src="/_next/image?url=%2Fpublications%2FMemory-augmented.png&amp;w=3840&amp;q=75"/></div><div class="md:w-2/3"><a target="_blank" rel="noopener noreferrer" class="group" href="https://ieeexplore.ieee.org/abstract/document/9664442"><h3 class="text-lg font-medium mb-3 group-hover:text-blue-600 transition-colors">Memory-augmented generative adversarial networks for anomaly detection</h3></a><div class="flex flex-wrap gap-2 mb-3 text-sm text-gray-600"><span>Z Yang<!-- -->,</span><span>T Zhang<!-- -->,</span><span>IS Bozchalooi<!-- -->,</span><span>E Darve</span></div><div class="flex gap-4 text-sm"><span class="text-gray-600">IEEE Transactions on Neural Networks and Learning Systems 33 (6), 2324-2334</span><span class="text-gray-500 capitalize">journal</span><span class="text-gray-600">2021</span></div></div></div><div class="flex flex-col md:flex-row gap-8 border-b border-gray-100 pb-12 last:border-0"><div class="md:w-1/3 aspect-video relative overflow-hidden rounded-lg bg-gray-100"><video class="w-full h-full object-cover" poster="/publications/regularized.png" autoPlay="" muted="" loop="" playsInline="" preload="auto"><source src="/publications/regularized.png" type="video/mp4"/></video></div><div class="md:w-2/3"><a target="_blank" rel="noopener noreferrer" class="group" href="https://ebooks.iospress.nl/volumearticle/55067"><h3 class="text-lg font-medium mb-3 group-hover:text-blue-600 transition-colors">Regularized cycle consistent generative adversarial network for anomaly detection</h3></a><div class="flex flex-wrap gap-2 mb-3 text-sm text-gray-600"><span>Z Yang<!-- -->,</span><span>I Soltani Bozchalooi<!-- -->,</span><span>E Darve</span></div><div class="flex gap-4 text-sm"><span class="text-gray-600">ECAI 2020, 1618-1625</span><span class="text-gray-500 capitalize">conference</span><span class="text-gray-600">2020</span></div></div></div><div class="flex flex-col md:flex-row gap-8 border-b border-gray-100 pb-12 last:border-0"><div class="md:w-1/3 aspect-video relative overflow-hidden rounded-lg bg-gray-100"><video class="w-full h-full object-cover" poster="/publications/jvb.jpg" autoPlay="" muted="" loop="" playsInline="" preload="auto"><source src="/publications/jvb.jpg" type="video/mp4"/></video></div><div class="md:w-2/3"><a target="_blank" rel="noopener noreferrer" class="group" href="https://pubs.aip.org/avs/jvb/article/35/6/06G101/591631"><h3 class="text-lg font-medium mb-3 group-hover:text-blue-600 transition-colors">Active scanning probes: A versatile toolkit for fast imaging and emerging nanofabrication</h3></a><div class="flex flex-wrap gap-2 mb-3 text-sm text-gray-600"><span>IW Rangelow<!-- -->,</span><span>T Ivanov<!-- -->,</span><span>A Ahmad<!-- -->,</span><span>M Kaestner<!-- -->,</span><span>C Lenk<!-- -->,</span><span>IS Bozchalooi<!-- -->,</span><span>F Xia<!-- -->,</span><span>K Youcef-Toumi<!-- -->,</span><span>M Holz<!-- -->,</span><span>A Reum</span></div><div class="flex gap-4 text-sm"><span class="text-gray-600">Journal of Vacuum Science &amp; Technology B 35 (6), 06G101</span><span class="text-gray-500 capitalize">journal</span><span class="text-gray-600">2017</span></div></div></div><div class="flex flex-col md:flex-row gap-8 border-b border-gray-100 pb-12 last:border-0"><div class="md:w-1/3 aspect-video relative overflow-hidden rounded-lg bg-gray-100"><img alt="Induced vibration contact detection for minimizing cantilever tip-sample interaction forces in jumping mode atomic force microscopy" loading="lazy" decoding="async" data-nimg="fill" class="object-cover" style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;color:transparent" sizes="100vw" srcSet="/_next/image?url=%2Fpublications%2FInduced-vibration.png&amp;w=640&amp;q=75 640w, /_next/image?url=%2Fpublications%2FInduced-vibration.png&amp;w=750&amp;q=75 750w, /_next/image?url=%2Fpublications%2FInduced-vibration.png&amp;w=828&amp;q=75 828w, /_next/image?url=%2Fpublications%2FInduced-vibration.png&amp;w=1080&amp;q=75 1080w, /_next/image?url=%2Fpublications%2FInduced-vibration.png&amp;w=1200&amp;q=75 1200w, /_next/image?url=%2Fpublications%2FInduced-vibration.png&amp;w=1920&amp;q=75 1920w, /_next/image?url=%2Fpublications%2FInduced-vibration.png&amp;w=2048&amp;q=75 2048w, /_next/image?url=%2Fpublications%2FInduced-vibration.png&amp;w=3840&amp;q=75 3840w" src="/_next/image?url=%2Fpublications%2FInduced-vibration.png&amp;w=3840&amp;q=75"/></div><div class="md:w-2/3"><a target="_blank" rel="noopener noreferrer" class="group" href="https://ieeexplore.ieee.org/abstract/document/7963591"><h3 class="text-lg font-medium mb-3 group-hover:text-blue-600 transition-colors">Induced vibration contact detection for minimizing cantilever tip-sample interaction forces in jumping mode atomic force microscopy</h3></a><div class="flex flex-wrap gap-2 mb-3 text-sm text-gray-600"><span>F Xia<!-- -->,</span><span>I Soltani Bozchalooi<!-- -->,</span><span>K Youcef-Toumi</span></div><div class="flex gap-4 text-sm"><span class="text-gray-600">2017 American Control Conference (ACC)</span><span class="text-gray-500 capitalize">conference</span><span class="text-gray-600">2017</span></div></div></div><div class="flex flex-col md:flex-row gap-8 border-b border-gray-100 pb-12 last:border-0"><div class="md:w-1/3 aspect-video relative overflow-hidden rounded-lg bg-gray-100"><video class="w-full h-full object-cover" poster="/publications/designandcontrol.mp4" autoPlay="" muted="" loop="" playsInline="" preload="auto"><source src="/publications/designandcontrol.mp4" type="video/mp4"/></video></div><div class="md:w-2/3"><a target="_blank" rel="noopener noreferrer" class="group" href="https://www.sciencedirect.com/science/article/pii/S0304399115300528"><h3 class="text-lg font-medium mb-3 group-hover:text-blue-600 transition-colors">Design and control of multi-actuated atomic force microscope for large-range and high-speed imaging</h3></a><div class="flex flex-wrap gap-2 mb-3 text-sm text-gray-600"><span>I Soltani Bozchalooi<!-- -->,</span><span>A Careaga Houck<!-- -->,</span><span>JM AlGhamdi<!-- -->,</span><span>K Youcef-Toumi</span></div><div class="flex gap-4 text-sm"><span class="text-gray-600">Ultramicroscopy 160, 213-224</span><span class="text-gray-500 capitalize">journal</span><span class="text-gray-600">2016</span></div></div></div><div class="flex flex-col md:flex-row gap-8 border-b border-gray-100 pb-12 last:border-0"><div class="md:w-1/3 aspect-video relative overflow-hidden rounded-lg bg-gray-100"><img alt="A study on the effectiveness of proportional-integral-derivative control in multi-actuated atomic force microscopy" loading="lazy" decoding="async" data-nimg="fill" class="object-cover" style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;color:transparent" sizes="100vw" srcSet="/_next/image?url=%2Fpublications%2FInformation-guided.png&amp;w=640&amp;q=75 640w, /_next/image?url=%2Fpublications%2FInformation-guided.png&amp;w=750&amp;q=75 750w, /_next/image?url=%2Fpublications%2FInformation-guided.png&amp;w=828&amp;q=75 828w, /_next/image?url=%2Fpublications%2FInformation-guided.png&amp;w=1080&amp;q=75 1080w, /_next/image?url=%2Fpublications%2FInformation-guided.png&amp;w=1200&amp;q=75 1200w, /_next/image?url=%2Fpublications%2FInformation-guided.png&amp;w=1920&amp;q=75 1920w, /_next/image?url=%2Fpublications%2FInformation-guided.png&amp;w=2048&amp;q=75 2048w, /_next/image?url=%2Fpublications%2FInformation-guided.png&amp;w=3840&amp;q=75 3840w" src="/_next/image?url=%2Fpublications%2FInformation-guided.png&amp;w=3840&amp;q=75"/></div><div class="md:w-2/3"><a target="_blank" rel="noopener noreferrer" class="group" href="https://ieeexplore.ieee.org/document/7171869"><h3 class="text-lg font-medium mb-3 group-hover:text-blue-600 transition-colors">A study on the effectiveness of proportional-integral-derivative control in multi-actuated atomic force microscopy</h3></a><div class="flex flex-wrap gap-2 mb-3 text-sm text-gray-600"><span>I Soltani Bozchalooi<!-- -->,</span><span>AC Houck<!-- -->,</span><span>K Youcef-Toumi</span></div><div class="flex gap-4 text-sm"><span class="text-gray-600">2015 American Control Conference (ACC)</span><span class="text-gray-500 capitalize">conference</span><span class="text-gray-600">2015</span></div></div></div><div class="flex flex-col md:flex-row gap-8 border-b border-gray-100 pb-12 last:border-0"><div class="md:w-1/3 aspect-video relative overflow-hidden rounded-lg bg-gray-100"><img alt="Estimator based multi-eigenmode control of cantilevers in multifrequency Atomic Force Microscopy" loading="lazy" decoding="async" data-nimg="fill" class="object-cover" style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;color:transparent" sizes="100vw" srcSet="/_next/image?url=%2Fpublications%2FEstimator-based.png&amp;w=640&amp;q=75 640w, /_next/image?url=%2Fpublications%2FEstimator-based.png&amp;w=750&amp;q=75 750w, /_next/image?url=%2Fpublications%2FEstimator-based.png&amp;w=828&amp;q=75 828w, /_next/image?url=%2Fpublications%2FEstimator-based.png&amp;w=1080&amp;q=75 1080w, /_next/image?url=%2Fpublications%2FEstimator-based.png&amp;w=1200&amp;q=75 1200w, /_next/image?url=%2Fpublications%2FEstimator-based.png&amp;w=1920&amp;q=75 1920w, /_next/image?url=%2Fpublications%2FEstimator-based.png&amp;w=2048&amp;q=75 2048w, /_next/image?url=%2Fpublications%2FEstimator-based.png&amp;w=3840&amp;q=75 3840w" src="/_next/image?url=%2Fpublications%2FEstimator-based.png&amp;w=3840&amp;q=75"/></div><div class="md:w-2/3"><a target="_blank" rel="noopener noreferrer" class="group" href="https://ieeexplore.ieee.org/abstract/document/7171011"><h3 class="text-lg font-medium mb-3 group-hover:text-blue-600 transition-colors">Estimator based multi-eigenmode control of cantilevers in multifrequency Atomic Force Microscopy</h3></a><div class="flex flex-wrap gap-2 mb-3 text-sm text-gray-600"><span>A Schuh<!-- -->,</span><span>I Soltani Bozchalooi<!-- -->,</span><span>IW Rangelow<!-- -->,</span><span>K Youcef-Toumi</span></div><div class="flex gap-4 text-sm"><span class="text-gray-600">American Control Conference (ACC)</span><span class="text-gray-500 capitalize">conference</span><span class="text-gray-600">2015</span></div></div></div></div><footer class="mt-24 border-t border-gray-100 w-full"><div class="py-12"><div class="flex flex-col space-y-8"><div class="flex flex-col md:flex-row md:justify-between gap-4 ml-4 text-[13px]"><div class="space-y-1 text-gray-600"><p>University of California, Davis</p><p>One Shields Avenue, Davis, CA 95616</p></div><div class="flex items-center gap-6 text-gray-400"><a href="/contact" class="hover:text-gray-900 transition-colors">Contact</a><a href="/faq" class="hover:text-gray-900 transition-colors">FAQ</a><span>Â© <!-- -->2025</span></div></div><div class="flex flex-col md:flex-row md:justify-between items-start md:items-center gap-2 text-[11px] ml-4 text-gray-400"><span>Last updated: <!-- -->December 6, 2025</span><div class="flex items-center gap-1"><span>Built by</span><a href="https://mohnishgopi.com" target="_blank" rel="noopener noreferrer" class="text-gray-600 hover:text-gray-900 transition-colors">Mohnish Gopi</a></div></div></div></div></footer></div></div></main></div></main></div></div><script src="/_next/static/chunks/webpack-b61c5b23e818b40e.js" async=""></script><script>(self.__next_f=self.__next_f||[]).push([0]);self.__next_f.push([2,null])</script><script>self.__next_f.push([1,"1:HL[\"/_next/static/media/e11418ac562b8ac1-s.p.woff2\",\"font\",{\"crossOrigin\":\"\",\"type\":\"font/woff2\"}]\n2:HL[\"/_next/static/css/ae2e9ad0d33ab6d9.css\",\"style\"]\n"])</script><script>self.__next_f.push([1,"3:I[5751,[],\"\"]\n5:I[6513,[],\"ClientPageRoot\"]\n6:I[6993,[\"173\",\"static/chunks/173-1b457355cc0f6eb8.js\",\"231\",\"static/chunks/231-75217c32967b80fc.js\",\"474\",\"static/chunks/474-3830407772278304.js\",\"304\",\"static/chunks/app/publications/page-fe2c473f5f919a5b.js\"],\"default\"]\n7:I[9275,[],\"\"]\n8:I[1343,[],\"\"]\n9:I[4858,[\"699\",\"static/chunks/8e1d74a4-561e044f358a70a9.js\",\"173\",\"static/chunks/173-1b457355cc0f6eb8.js\",\"231\",\"static/chunks/231-75217c32967b80fc.js\",\"521\",\"static/chunks/521-a3cc0266cc1077a1.js\",\"443\",\"static/chunks/443-ccb272207b22f4bd.js\",\"185\",\"static/chunks/app/layout-e9f41310a2e3199e.js\"],\"ThemeProvider\"]\na:I[9736,[\"699\",\"static/chunks/8e1d74a4-561e044f358a70a9.js\",\"173\",\"static/chunks/173-1b457355cc0f6eb8.js\",\"231\",\"static/chunks/231-75217c32967b80fc.js\",\"521\",\"static/chunks/521-a3cc0266cc1077a1.js\",\"443\",\"static/chunks/443-ccb272207b22f4bd.js\",\"185\",\"static/chunks/app/layout-e9f41310a2e3199e.js\"],\"TooltipProvider\"]\nb:I[1807,[\"699\",\"static/chunks/8e1d74a4-561e044f358a70a9.js\",\"173\",\"static/chunks/173-1b457355cc0f6eb8.js\",\"231\",\"static/chunks/231-75217c32967b80fc.js\",\"521\",\"static/chunks/521-a3cc0266cc1077a1.js\",\"443\",\"static/chunks/443-ccb272207b22f4bd.js\",\"185\",\"static/chunks/app/layout-e9f41310a2e3199e.js\"],\"default\"]\nc:I[231,[\"231\",\"static/chunks/231-75217c32967b80fc.js\",\"160\",\"static/chunks/app/not-found-687fa6ca5be28a47.js\"],\"\"]\ne:I[6130,[],\"\"]\nf:[]\n"])</script><script>self.__next_f.push([1,"0:[[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/css/ae2e9ad0d33ab6d9.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\"}]],[\"$\",\"$L3\",null,{\"buildId\":\"iYG3uapcOk9H8a6sWXgmA\",\"assetPrefix\":\"\",\"initialCanonicalUrl\":\"/publications\",\"initialTree\":[\"\",{\"children\":[\"publications\",{\"children\":[\"__PAGE__\",{}]}]},\"$undefined\",\"$undefined\",true],\"initialSeedData\":[\"\",{\"children\":[\"publications\",{\"children\":[\"__PAGE__\",{},[[\"$L4\",[\"$\",\"$L5\",null,{\"props\":{\"params\":{},\"searchParams\":{}},\"Component\":\"$6\"}]],null],null]},[\"$\",\"$L7\",null,{\"parallelRouterKey\":\"children\",\"segmentPath\":[\"children\",\"publications\",\"children\"],\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L8\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"notFoundStyles\":\"$undefined\",\"styles\":null}],null]},[[\"$\",\"html\",null,{\"lang\":\"en\",\"suppressHydrationWarning\":true,\"className\":\"__className_3a0388\",\"children\":[\"$\",\"body\",null,{\"className\":\"antialiased bg-gray-50\",\"children\":[\"$\",\"$L9\",null,{\"attribute\":\"class\",\"defaultTheme\":\"light\",\"children\":[\"$\",\"$La\",null,{\"delayDuration\":0,\"children\":[\"$\",\"div\",null,{\"className\":\"min-h-screen p-3\",\"children\":[\"$\",\"div\",null,{\"className\":\"fixed inset-3 flex rounded-xl bg-white shadow-sm\",\"children\":[[\"$\",\"$Lb\",null,{}],[\"$\",\"main\",null,{\"className\":\"flex-1 overflow-auto\",\"children\":[\"$\",\"div\",null,{\"className\":\"w-full\",\"children\":[\"$\",\"$L7\",null,{\"parallelRouterKey\":\"children\",\"segmentPath\":[\"children\"],\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L8\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":[\"$\",\"main\",null,{\"className\":\"flex flex-col min-h-screen\",\"children\":[\"$\",\"div\",null,{\"className\":\"flex-grow flex items-center justify-center px-6 md:px-10\",\"children\":[\"$\",\"div\",null,{\"className\":\"max-w-md mx-auto text-center\",\"children\":[[\"$\",\"h1\",null,{\"className\":\"text-4xl font-medium mb-4\",\"children\":\"404\"}],[\"$\",\"h2\",null,{\"className\":\"text-xl mb-4\",\"children\":\"Page Not Found\"}],[\"$\",\"p\",null,{\"className\":\"text-gray-600 mb-8\",\"children\":\"The page you're looking for doesn't exist or has been moved.\"}],[\"$\",\"$Lc\",null,{\"href\":\"/\",\"className\":\"inline-flex items-center text-sm bg-black text-white px-6 py-3 rounded-full hover:bg-gray-800 transition-colors\",\"children\":\"Return Home\"}]]}]}]}],\"notFoundStyles\":[],\"styles\":null}]}]}]]}]}]}]}]}]}],null],null],\"couldBeIntercepted\":false,\"initialHead\":[false,\"$Ld\"],\"globalErrorComponent\":\"$e\",\"missingSlots\":\"$Wf\"}]]\n"])</script><script>self.__next_f.push([1,"d:[[\"$\",\"meta\",\"0\",{\"name\":\"viewport\",\"content\":\"width=device-width, initial-scale=1\"}],[\"$\",\"meta\",\"1\",{\"charSet\":\"utf-8\"}],[\"$\",\"title\",\"2\",{\"children\":\"LARA\"}],[\"$\",\"meta\",\"3\",{\"name\":\"description\",\"content\":\"The Laboratory for AI, Robotics, and Automation (LARA) at UC Davis is at the forefront of research in artificial intelligence and robotics, focusing on the development of autonomous systems that can learn, adapt, and operate in complex environments. Our interdisciplinary research spans across cutting-edge technologies, including machine learning, reinforcement learning, and robotic systems, with applications in automation, autonomous vehicles, and human-robot interaction.\"}],[\"$\",\"meta\",\"4\",{\"name\":\"robots\",\"content\":\"index, follow\"}],[\"$\",\"meta\",\"5\",{\"name\":\"googlebot\",\"content\":\"index, follow, max-video-preview:-1, max-image-preview:large, max-snippet:-1\"}],[\"$\",\"meta\",\"6\",{\"property\":\"og:title\",\"content\":\"LARA\"}],[\"$\",\"meta\",\"7\",{\"property\":\"og:description\",\"content\":\"The Laboratory for AI, Robotics, and Automation (LARA) at UC Davis is at the forefront of research in artificial intelligence and robotics, focusing on the development of autonomous systems that can learn, adapt, and operate in complex environments. Our interdisciplinary research spans across cutting-edge technologies, including machine learning, reinforcement learning, and robotic systems, with applications in automation, autonomous vehicles, and human-robot interaction.\"}],[\"$\",\"meta\",\"8\",{\"property\":\"og:url\",\"content\":\"https://dillion.io\"}],[\"$\",\"meta\",\"9\",{\"property\":\"og:site_name\",\"content\":\"LARA\"}],[\"$\",\"meta\",\"10\",{\"property\":\"og:locale\",\"content\":\"en_US\"}],[\"$\",\"meta\",\"11\",{\"property\":\"og:type\",\"content\":\"website\"}],[\"$\",\"meta\",\"12\",{\"name\":\"twitter:card\",\"content\":\"summary_large_image\"}],[\"$\",\"meta\",\"13\",{\"name\":\"twitter:title\",\"content\":\"LARA\"}],[\"$\",\"meta\",\"14\",{\"name\":\"twitter:description\",\"content\":\"The Laboratory for AI, Robotics, and Automation (LARA) at UC Davis is at the forefront of research in artificial intelligence and robotics, focusing on the development of autonomous systems that can learn, adapt, and operate in complex environments. Our interdisciplinary research spans across cutting-edge technologies, including machine learning, reinforcement learning, and robotic systems, with applications in automation, autonomous vehicles, and human-robot interaction.\"}],[\"$\",\"link\",\"15\",{\"rel\":\"icon\",\"href\":\"/favicon.ico\",\"type\":\"image/x-icon\",\"sizes\":\"16x16\"}],[\"$\",\"meta\",\"16\",{\"name\":\"next-size-adjust\"}]]\n"])</script><script>self.__next_f.push([1,"4:null\n"])</script></body></html>