<!DOCTYPE html><html lang="en" class="__className_3a0388"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width, initial-scale=1"/><link rel="preload" href="/_next/static/media/e11418ac562b8ac1-s.p.woff2" as="font" crossorigin="" type="font/woff2"/><link rel="stylesheet" href="/_next/static/css/ae2e9ad0d33ab6d9.css" data-precedence="next"/><link rel="preload" as="script" fetchPriority="low" href="/_next/static/chunks/webpack-b61c5b23e818b40e.js"/><script src="/_next/static/chunks/fd9d1056-bb11881ef41582aa.js" async=""></script><script src="/_next/static/chunks/23-d4169bb1b2b13904.js" async=""></script><script src="/_next/static/chunks/main-app-bde93d12e61991d7.js" async=""></script><script src="/_next/static/chunks/173-8181ae1d7cdec007.js" async=""></script><script src="/_next/static/chunks/231-75217c32967b80fc.js" async=""></script><script src="/_next/static/chunks/521-4c58f4a3393261e6.js" async=""></script><script src="/_next/static/chunks/474-576ff5c0c60b07ac.js" async=""></script><script src="/_next/static/chunks/app/page-aea081ca658e47e3.js" async=""></script><script src="/_next/static/chunks/8e1d74a4-561e044f358a70a9.js" async=""></script><script src="/_next/static/chunks/443-ccb272207b22f4bd.js" async=""></script><script src="/_next/static/chunks/app/layout-e9f41310a2e3199e.js" async=""></script><script src="/_next/static/chunks/app/not-found-687fa6ca5be28a47.js" async=""></script><title>LARA</title><meta name="description" content="The Laboratory for AI, Robotics, and Automation (LARA) at UC Davis is at the forefront of research in artificial intelligence and robotics, focusing on the development of autonomous systems that can learn, adapt, and operate in complex environments. Our interdisciplinary research spans across cutting-edge technologies, including machine learning, reinforcement learning, and robotic systems, with applications in automation, autonomous vehicles, and human-robot interaction."/><meta name="robots" content="index, follow"/><meta name="googlebot" content="index, follow, max-video-preview:-1, max-image-preview:large, max-snippet:-1"/><meta property="og:title" content="LARA"/><meta property="og:description" content="The Laboratory for AI, Robotics, and Automation (LARA) at UC Davis is at the forefront of research in artificial intelligence and robotics, focusing on the development of autonomous systems that can learn, adapt, and operate in complex environments. Our interdisciplinary research spans across cutting-edge technologies, including machine learning, reinforcement learning, and robotic systems, with applications in automation, autonomous vehicles, and human-robot interaction."/><meta property="og:url" content="https://dillion.io"/><meta property="og:site_name" content="LARA"/><meta property="og:locale" content="en_US"/><meta property="og:type" content="website"/><meta name="twitter:card" content="summary_large_image"/><meta name="twitter:title" content="LARA"/><meta name="twitter:description" content="The Laboratory for AI, Robotics, and Automation (LARA) at UC Davis is at the forefront of research in artificial intelligence and robotics, focusing on the development of autonomous systems that can learn, adapt, and operate in complex environments. Our interdisciplinary research spans across cutting-edge technologies, including machine learning, reinforcement learning, and robotic systems, with applications in automation, autonomous vehicles, and human-robot interaction."/><link rel="icon" href="/favicon.ico" type="image/x-icon" sizes="16x16"/><meta name="next-size-adjust"/><script src="/_next/static/chunks/polyfills-78c92fac7aa8fdd8.js" noModule=""></script></head><body class="antialiased bg-gray-50"><script>!function(){try{var d=document.documentElement,c=d.classList;c.remove('light','dark');var e=localStorage.getItem('theme');if('system'===e||(!e&&false)){var t='(prefers-color-scheme: dark)',m=window.matchMedia(t);if(m.media!==t||m.matches){d.style.colorScheme = 'dark';c.add('dark')}else{d.style.colorScheme = 'light';c.add('light')}}else if(e){c.add(e|| '')}else{c.add('light')}if(e==='light'||e==='dark'||!e)d.style.colorScheme=e||'light'}catch(e){}}()</script><div class="min-h-screen p-3"><div class="fixed inset-3 flex rounded-xl bg-white shadow-sm"><button class="fixed top-4 right-4 z-50 p-2 rounded-md bg-white shadow-sm sm:hidden"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-menu"><line x1="4" x2="20" y1="12" y2="12"></line><line x1="4" x2="20" y1="6" y2="6"></line><line x1="4" x2="20" y1="18" y2="18"></line></svg></button><aside class="fixed inset-y-0 left-0 z-40 w-64 bg-white transform transition-transform duration-200 ease-in-out sm:relative sm:transform-none sm:w-44 sm:border-r sm:border-gray-100 -translate-x-full sm:translate-x-0"><div class="sticky top-0 flex h-full flex-col justify-between py-2"><div><a class="flex items-center justify-center px-5 pt-8 pb-12" href="/"><img alt="LARA Lab" loading="lazy" width="140" height="140" decoding="async" data-nimg="1" style="color:transparent" src="/LARALOGO.png"/></a><nav class="space-y-1 px-2 mb-8"><a class="flex items-center px-3 py-2 text-[13px] transition-all duration-200 rounded-md hover:scale-105 hover:bg-gray-50 text-black font-medium bg-gray-50" href="/">Home</a><a class="flex items-center px-3 py-2 text-[13px] transition-all duration-200 rounded-md hover:scale-105 hover:bg-gray-50 text-gray-500 hover:text-gray-900" href="/about">About</a><a class="flex items-center px-3 py-2 text-[13px] transition-all duration-200 rounded-md hover:scale-105 hover:bg-gray-50 text-gray-500 hover:text-gray-900" href="/publications">Publications</a><a class="flex items-center px-3 py-2 text-[13px] transition-all duration-200 rounded-md hover:scale-105 hover:bg-gray-50 text-gray-500 hover:text-gray-900" href="/patents">Patents</a><a class="flex items-center px-3 py-2 text-[13px] transition-all duration-200 rounded-md hover:scale-105 hover:bg-gray-50 text-gray-500 hover:text-gray-900" href="/blog">News</a><a class="flex items-center px-3 py-2 text-[13px] transition-all duration-200 rounded-md hover:scale-105 hover:bg-gray-50 text-gray-500 hover:text-gray-900" href="/people">People</a><a class="flex items-center px-3 py-2 text-[13px] transition-all duration-200 rounded-md hover:scale-105 hover:bg-gray-50 text-gray-500 hover:text-gray-900" href="/faq">FAQ</a><a class="flex items-center px-3 py-2 text-[13px] transition-all duration-200 rounded-md hover:scale-105 hover:bg-gray-50 text-gray-500 hover:text-gray-900" href="/contact">Contact</a></nav></div><div class="space-y-8 px-5 pb-6"><a href="https://drive.google.com/file/d/1PuFn_1D4t18b7H_9VPRS3ilNOY-dlGjO/view" target="_blank" rel="noopener noreferrer" class="group flex items-center gap-2 text-[11px] text-gray-400 hover:text-gray-900 transition-colors"><span>View brand guidelines</span><svg xmlns="http://www.w3.org/2000/svg" width="10" height="10" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-arrow-right transition-transform group-hover:translate-x-0.5"><path d="M5 12h14"></path><path d="m12 5 7 7-7 7"></path></svg></a><div class="flex gap-4"><a href="https://linkedin.com" target="_blank" rel="noopener noreferrer" class="text-gray-400 hover:text-gray-900 transition-all hover:scale-110 duration-200" aria-label="LinkedIn"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 448 512" height="15" width="15" xmlns="http://www.w3.org/2000/svg"><path d="M416 32H31.9C14.3 32 0 46.5 0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6 0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3zM135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3 0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2 0 38.5 17.3 38.5 38.5 0 21.3-17.2 38.5-38.5 38.5zm282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6 0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2 0 79.7 44.3 79.7 101.9V416z"></path></svg></a><a href="https://scholar.google.com" target="_blank" rel="noopener noreferrer" class="text-gray-400 hover:text-gray-900 transition-all hover:scale-110 duration-200" aria-label="Google Scholar"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 488 512" height="15" width="15" xmlns="http://www.w3.org/2000/svg"><path d="M488 261.8C488 403.3 391.1 504 248 504 110.8 504 0 393.2 0 256S110.8 8 248 8c66.8 0 123 24.5 166.3 64.9l-67.5 64.9C258.5 52.6 94.3 116.6 94.3 256c0 86.5 69.1 156.6 153.7 156.6 98.2 0 135-70.4 140.8-106.9H248v-85.3h236.1c2.3 12.7 3.9 24.9 3.9 41.4z"></path></svg></a><a href="https://github.com" target="_blank" rel="noopener noreferrer" class="text-gray-400 hover:text-gray-900 transition-all hover:scale-110 duration-200" aria-label="GitHub"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 496 512" height="15" width="15" xmlns="http://www.w3.org/2000/svg"><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"></path></svg></a><a href="mailto:isoltani@ucdavis.edu" class="text-gray-400 hover:text-gray-900 transition-all hover:scale-110 duration-200" aria-label="Email"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 512 512" height="15" width="15" xmlns="http://www.w3.org/2000/svg"><path d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"></path></svg></a></div></div></div></aside><main class="flex-1 overflow-auto"><div class="w-full"><main class="flex flex-col"><section class="pt-32 pb-24 px-6 md:px-10"><div class="max-w-4xl mx-auto"><h1 class="text-5xl md:text-6xl font-medium tracking-tight mb-8 leading-[1.1]">Laboratory for AI,<br/>Robotics, and Automation</h1><p class="text-lg text-gray-600 mb-12 leading-relaxed max-w-2xl">Developing autonomous systems that learn, adapt, and operate in complex environments at UC Davis.</p><a class="inline-flex items-center text-[15px] font-medium bg-black text-white px-6 py-3 rounded-full hover:bg-gray-800 transition-all hover:translate-x-1" href="/contact">Get Involved <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-chevron-right ml-2 h-4 w-4"><path d="m9 18 6-6-6-6"></path></svg></a></div></section><section class="relative w-full h-[60vh] mb-24"><div class="absolute inset-0 bg-gradient-to-b from-transparent to-white/10 z-10"></div><img alt="UC Davis Engineering Building" loading="lazy" decoding="async" data-nimg="fill" class="object-cover" style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;color:transparent" src="/h2.jpg"/></section><section class="px-6 md:px-10 pb-24"><div class="max-w-4xl mx-auto"><h2 class="text-sm font-medium mb-16 uppercase tracking-wider text-gray-500">Research Focus</h2><div class="grid grid-cols-1 md:grid-cols-3 gap-16"><div class="group space-y-4"><div class="text-gray-400 group-hover:text-black transition-colors"><svg class="w-6 h-6" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="1.5"><path d="M21 12a9 9 0 11-18 0 9 9 0 0118 0z"></path><path d="M9 12l2 2 4-4"></path></svg></div><h3 class="text-xl font-medium">Autonomous Systems</h3><p class="text-gray-600 leading-relaxed">Developing intelligent systems that can operate independently in complex environments</p></div><div class="group space-y-4"><div class="text-gray-400 group-hover:text-black transition-colors"><svg class="w-6 h-6" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="1.5"><path d="M7 8h10M7 12h4m1 8l-4-4H5a2 2 0 01-2-2V6a2 2 0 012-2h14a2 2 0 012 2v8a2 2 0 01-2 2h-3l-4 4z"></path></svg></div><h3 class="text-xl font-medium">Industrial Diagnostics</h3><p class="text-gray-600 leading-relaxed">Creating solutions that enhance efficiency and safety in manufacturing and industrial processes</p></div><div class="group space-y-4"><div class="text-gray-400 group-hover:text-black transition-colors"><svg class="w-6 h-6" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="1.5"><path d="M9 17v-2m3 2v-4m3 4v-6m2 10H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"></path></svg></div><h3 class="text-xl font-medium">Health Diagnostics</h3><p class="text-gray-600 leading-relaxed">Applying AI and robotics to improve medical diagnostics and healthcare delivery systems</p></div></div></div></section><section class="px-6 md:px-10 py-24 bg-gray-50"><div class="max-w-4xl mx-auto"><div class="flex justify-between items-center mb-16"><h2 class="text-sm font-medium uppercase tracking-wider text-gray-500">Latest Research</h2><a class="text-sm font-medium hover:text-gray-600 transition-colors" href="/publications">View all publications →</a></div><div class="grid grid-cols-1 md:grid-cols-3 gap-12"><a target="_blank" rel="noopener noreferrer" class="group" href="https://arxiv.org/abs/2510.08442"><div class="space-y-6"><div class="aspect-video relative overflow-hidden rounded-lg bg-gray-100"><video class="w-full h-full object-cover" poster="/publications/gaze-on-the-prize.gif" autoPlay="" muted="" loop="" playsInline="" preload="auto"><source src="/publications/gaze-on-the-prize.gif" type="video/mp4"/></video></div><div><h3 class="text-lg font-medium mb-2 group-hover:text-blue-600 transition-colors">Gaze on the Prize: Shaping Visual Attention with Return-Guided Contrastive Learning</h3><div class="text-sm text-gray-600 mb-2"><span>Andrew Lee<!-- -->, </span><span>Ian Chuang<!-- -->, </span><span>Dechen Gao<!-- -->, </span><span>Kai Fukazawa<!-- -->, </span><span>Iman Soltani</span></div><div class="flex gap-4 text-sm"><span class="text-gray-600">arXiv preprint</span><span class="text-gray-500 capitalize">preprint</span><span class="text-gray-600">2025</span></div></div></div></a><a target="_blank" rel="noopener noreferrer" class="group" href="https://arxiv.org/abs/2510.02695"><div class="space-y-6"><div class="aspect-video relative overflow-hidden rounded-lg bg-gray-100"><img alt="RAMAC: Multimodal Risk-Aware Offline Reinforcement Learning and the Role of Behavior Regularization" loading="lazy" decoding="async" data-nimg="fill" class="object-cover" style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;color:transparent" src="/publications/"/></div><div><h3 class="text-lg font-medium mb-2 group-hover:text-blue-600 transition-colors">RAMAC: Multimodal Risk-Aware Offline Reinforcement Learning and the Role of Behavior Regularization</h3><div class="text-sm text-gray-600 mb-2"><span>Kai Fukazawa<!-- -->, </span><span>Kunal Mundada<!-- -->, </span><span>Iman Soltani</span></div><div class="flex gap-4 text-sm"><span class="text-gray-600">arXiv preprint</span><span class="text-gray-500 capitalize">preprint</span><span class="text-gray-600">2025</span></div></div></div></a><a target="_blank" rel="noopener noreferrer" class="group" href="https://arxiv.org/abs/2507.15833"><div class="space-y-6"><div class="aspect-video relative overflow-hidden rounded-lg bg-gray-100"><video class="w-full h-full object-cover" poster="/publications/look-focus-act.gif" autoPlay="" muted="" loop="" playsInline="" preload="auto"><source src="/publications/look-focus-act.gif" type="video/mp4"/></video></div><div><h3 class="text-lg font-medium mb-2 group-hover:text-blue-600 transition-colors">Look, focus, act: Efficient and robust robot learning via human gaze and foveated vision transformers</h3><div class="text-sm text-gray-600 mb-2"><span>Ian Chuang<!-- -->, </span><span>Jinyu Zou<!-- -->, </span><span>Andrew Lee<!-- -->, </span><span>Dechen Gao<!-- -->, </span><span>Iman Soltani</span></div><div class="flex gap-4 text-sm"><span class="text-gray-600">arXiv preprint</span><span class="text-gray-500 capitalize">preprint</span><span class="text-gray-600">2025</span></div></div></div></a><a target="_blank" rel="noopener noreferrer" class="group" href="https://arxiv.org/abs/2507.13231"><div class="space-y-6"><div class="aspect-video relative overflow-hidden rounded-lg bg-gray-100"><video class="w-full h-full object-cover" poster="/publications/VITA.gif" autoPlay="" muted="" loop="" playsInline="" preload="auto"><source src="/publications/VITA.gif" type="video/mp4"/></video></div><div><h3 class="text-lg font-medium mb-2 group-hover:text-blue-600 transition-colors">VITA: Vision-to-Action Flow Matching Policy</h3><div class="text-sm text-gray-600 mb-2"><span>Dechen Gao<!-- -->, </span><span>Boqi Zhao<!-- -->, </span><span>Andrew Lee<!-- -->, </span><span>Ian Chuang<!-- -->, </span><span>Hanchu Zhou<!-- -->, </span><span>Hang Wang<!-- -->, </span><span>Zhe Zhao<!-- -->, </span><span>Junshan Zhang<!-- -->, </span><span>Iman Soltani</span></div><div class="flex gap-4 text-sm"><span class="text-gray-600">arXiv preprint</span><span class="text-gray-500 capitalize">preprint</span><span class="text-gray-600">2025</span></div></div></div></a><a target="_blank" rel="noopener noreferrer" class="group" href="https://iovs.arvojournals.org/article.aspx?articleid=2805754"><div class="space-y-6"><div class="aspect-video relative overflow-hidden rounded-lg bg-gray-100"><img alt="Predicting Retinal Inflammation in Uveitis using OCT-Based Machine Learning" loading="lazy" decoding="async" data-nimg="fill" class="object-cover" style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;color:transparent" src="/publications/"/></div><div><h3 class="text-lg font-medium mb-2 group-hover:text-blue-600 transition-colors">Predicting Retinal Inflammation in Uveitis using OCT-Based Machine Learning</h3><div class="text-sm text-gray-600 mb-2"><span>Parisa Emami-Naeini<!-- -->, </span><span>Amin Ghafourian<!-- -->, </span><span>Mohammad Mehdi Johari Moghadam<!-- -->, </span><span>Fateme Montazeri<!-- -->, </span><span>Iman Soltani</span></div><div class="flex gap-4 text-sm"><span class="text-gray-600">Investigative Ophthalmology &amp; Visual Science</span><span class="text-gray-500 capitalize">journal</span><span class="text-gray-600">2025</span></div></div></div></a><a target="_blank" rel="noopener noreferrer" class="group" href="https://journals.sagepub.com/doi/full/10.1177/00202940251346112"><div class="space-y-6"><div class="aspect-video relative overflow-hidden rounded-lg bg-gray-100"><img alt="Design and simulation of vehicle motion tracking system using a Youla controller output observation system" loading="lazy" decoding="async" data-nimg="fill" class="object-cover" style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;color:transparent" src="/publications/"/></div><div><h3 class="text-lg font-medium mb-2 group-hover:text-blue-600 transition-colors">Design and simulation of vehicle motion tracking system using a Youla controller output observation system</h3><div class="text-sm text-gray-600 mb-2"><span>Rongfei Li<!-- -->, </span><span>Francis F Assadian<!-- -->, </span><span>Iman Soltani</span></div><div class="flex gap-4 text-sm"><span class="text-gray-600">Measurement and Control</span><span class="text-gray-500 capitalize">journal</span><span class="text-gray-600">2025</span></div></div></div></a><a target="_blank" rel="noopener noreferrer" class="group" href="https://arxiv.org/abs/2505.10442"><div class="space-y-6"><div class="aspect-video relative overflow-hidden rounded-lg bg-gray-100"><video class="w-full h-full object-cover" poster="/publications/IN-RIL.gif" autoPlay="" muted="" loop="" playsInline="" preload="auto"><source src="/publications/IN-RIL.gif" type="video/mp4"/></video></div><div><h3 class="text-lg font-medium mb-2 group-hover:text-blue-600 transition-colors">IN-RIL: Interleaved Reinforcement and Imitation Learning for Policy Fine-Tuning</h3><div class="text-sm text-gray-600 mb-2"><span>Dechen Gao<!-- -->, </span><span>Hang Wang<!-- -->, </span><span>Hanchu Zhou<!-- -->, </span><span>Nejib Ammar<!-- -->, </span><span>Shatadal Mishra<!-- -->, </span><span>Ahmadreza Moradipari<!-- -->, </span><span>Iman Soltani<!-- -->, </span><span>Junshan Zhang</span></div><div class="flex gap-4 text-sm"><span class="text-gray-600">arXiv preprint</span><span class="text-gray-500 capitalize">preprint</span><span class="text-gray-600">2025</span></div></div></div></a><a target="_blank" rel="noopener noreferrer" class="group" href="https://arxiv.org/abs/2505.05752"><div class="space-y-6"><div class="aspect-video relative overflow-hidden rounded-lg bg-gray-100"><img alt="Automating Infrastructure Surveying: A Framework for Geometric Measurements and Compliance Assessment Using Point Cloud Data" loading="lazy" decoding="async" data-nimg="fill" class="object-cover" style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;color:transparent" src="/publications/Automating.png"/></div><div><h3 class="text-lg font-medium mb-2 group-hover:text-blue-600 transition-colors">Automating Infrastructure Surveying: A Framework for Geometric Measurements and Compliance Assessment Using Point Cloud Data</h3><div class="text-sm text-gray-600 mb-2"><span>Amin Ghafourian<!-- -->, </span><span>Andrew Lee<!-- -->, </span><span>Dechen Gao<!-- -->, </span><span>Tyler Beer<!-- -->, </span><span>Kin Yen<!-- -->, </span><span>Iman Soltani</span></div><div class="flex gap-4 text-sm"><span class="text-gray-600">arXiv preprint</span><span class="text-gray-500 capitalize">preprint</span><span class="text-gray-600">2025</span></div></div></div></a><a target="_blank" rel="noopener noreferrer" class="group" href="https://arxiv.org/abs/2504.12967"><div class="space-y-6"><div class="aspect-video relative overflow-hidden rounded-lg bg-gray-100"><video class="w-full h-full object-cover" poster="/publications/krysalis.mp4" autoPlay="" muted="" loop="" playsInline="" preload="auto"><source src="/publications/krysalis.mp4" type="video/mp4"/></video></div><div><h3 class="text-lg font-medium mb-2 group-hover:text-blue-600 transition-colors">Krysalis Hand: A Lightweight, High-Payload, 18-DoF Anthropomorphic End-Effector for Robotic Learning and Dexterous Manipulation</h3><div class="text-sm text-gray-600 mb-2"><span>A Basheer<!-- -->, </span><span>J Chang<!-- -->, </span><span>Y Chen<!-- -->, </span><span>D Kim<!-- -->, </span><span>I Soltani</span></div><div class="flex gap-4 text-sm"><span class="text-gray-600">arXiv preprint</span><span class="text-gray-500 capitalize">preprint</span><span class="text-gray-600">2025</span></div></div></div></a><a target="_blank" rel="noopener noreferrer" class="group" href="https://arxiv.org/abs/2504.06379"><div class="space-y-6"><div class="aspect-video relative overflow-hidden rounded-lg bg-gray-100"><img alt="User-Centered Insights into Assistive Navigation Technologies for Individuals with Visual Impairment" loading="lazy" decoding="async" data-nimg="fill" class="object-cover" style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;color:transparent" src="/publications/assistive.png"/></div><div><h3 class="text-lg font-medium mb-2 group-hover:text-blue-600 transition-colors">User-Centered Insights into Assistive Navigation Technologies for Individuals with Visual Impairment</h3><div class="text-sm text-gray-600 mb-2"><span>I Soltani<!-- -->, </span><span>J Schofield<!-- -->, </span><span>M Madani<!-- -->, </span><span>D Kish<!-- -->, </span><span>P Emami-Naeini</span></div><div class="flex gap-4 text-sm"><span class="text-gray-600">arXiv preprint</span><span class="text-gray-500 capitalize">preprint</span><span class="text-gray-600">2025</span></div></div></div></a><a target="_blank" rel="noopener noreferrer" class="group" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11180914"><div class="space-y-6"><div class="aspect-video relative overflow-hidden rounded-lg bg-gray-100"><video class="w-full h-full object-cover" poster="/publications/harsh1.mp4" autoPlay="" muted="" loop="" playsInline="" preload="auto"><source src="/publications/harsh1.mp4" type="video/mp4"/></video></div><div><h3 class="text-lg font-medium mb-2 group-hover:text-blue-600 transition-colors">Infrared Vision Systems for Emergency Vehicle Driver Assistance in Low-Visibility Conditions</h3><div class="text-sm text-gray-600 mb-2"><span>M-Mahdi Naddaf-Sh<!-- -->, </span><span>Andrew Lee<!-- -->, </span><span>Kin Yen<!-- -->, </span><span>Eemon Amini<!-- -->, </span><span>Iman Soltani</span></div><div class="flex gap-4 text-sm"><span class="text-gray-600">IEEE Access</span><span class="text-gray-500 capitalize">journal</span><span class="text-gray-600">2025</span></div></div></div></a><a target="_blank" rel="noopener noreferrer" class="group" href="https://arxiv.org/abs/2502.12539"><div class="space-y-6"><div class="aspect-video relative overflow-hidden rounded-lg bg-gray-100"><video class="w-full h-full object-cover" poster="/publications/dual.mp4" autoPlay="" muted="" loop="" playsInline="" preload="auto"><source src="/publications/dual.mp4" type="video/mp4"/></video></div><div><h3 class="text-lg font-medium mb-2 group-hover:text-blue-600 transition-colors">Design and Implementation of a Dual Uncrewed Surface Vessel Platform for Bathymetry Research under High-flow Conditions</h3><div class="text-sm text-gray-600 mb-2"><span>D Kumar<!-- -->, </span><span>A Ghorbanpour<!-- -->, </span><span>K Yen<!-- -->, </span><span>I Soltani</span></div><div class="flex gap-4 text-sm"><span class="text-gray-600">arXiv</span><span class="text-gray-500 capitalize">preprint</span><span class="text-gray-600">2025</span></div></div></div></a><a target="_blank" rel="noopener noreferrer" class="group" href="https://arxiv.org/abs/2409.17435"><div class="space-y-6"><div class="aspect-video relative overflow-hidden rounded-lg bg-gray-100"><img alt="Active vision might be all you need: Exploring active vision in bimanual robotic manipulation" loading="lazy" decoding="async" data-nimg="fill" class="object-cover" style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;color:transparent" src="/publications/av-aloha.gif"/></div><div><h3 class="text-lg font-medium mb-2 group-hover:text-blue-600 transition-colors">Active vision might be all you need: Exploring active vision in bimanual robotic manipulation</h3><div class="text-sm text-gray-600 mb-2"><span>I Chuang<!-- -->, </span><span>A Lee<!-- -->, </span><span>D Gao<!-- -->, </span><span>I Soltani</span></div><div class="flex gap-4 text-sm"><span class="text-gray-600">IEEE International Conference on Robotics and Automation (ICRA)</span><span class="text-gray-500 capitalize">conference</span><span class="text-gray-600">2025</span></div></div></div></a><a target="_blank" rel="noopener noreferrer" class="group" href="https://ieeexplore.ieee.org/abstract/document/10720395"><div class="space-y-6"><div class="aspect-video relative overflow-hidden rounded-lg bg-gray-100"><img alt="Targeted collapse regularized autoencoder for anomaly detection: black hole at the center" loading="lazy" decoding="async" data-nimg="fill" class="object-cover" style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;color:transparent" src="/publications/collapse.png"/></div><div><h3 class="text-lg font-medium mb-2 group-hover:text-blue-600 transition-colors">Targeted collapse regularized autoencoder for anomaly detection: black hole at the center</h3><div class="text-sm text-gray-600 mb-2"><span>A Ghafourian<!-- -->, </span><span>H Shui<!-- -->, </span><span>D Upadhyay<!-- -->, </span><span>R Gupta<!-- -->, </span><span>D Filev<!-- -->, </span><span>I Soltani</span></div><div class="flex gap-4 text-sm"><span class="text-gray-600">IEEE Transactions on Neural Networks and Learning Systems</span><span class="text-gray-500 capitalize">journal</span><span class="text-gray-600">2024</span></div></div></div></a><a target="_blank" rel="noopener noreferrer" class="group" href="https://ieeexplore.ieee.org/abstract/document/10714437"><div class="space-y-6"><div class="aspect-video relative overflow-hidden rounded-lg bg-gray-100"><video class="w-full h-full object-cover" poster="/publications/cardreamer.gif" autoPlay="" muted="" loop="" playsInline="" preload="auto"><source src="/publications/cardreamer.gif" type="video/mp4"/></video></div><div><h3 class="text-lg font-medium mb-2 group-hover:text-blue-600 transition-colors">Cardreamer: Open-source learning platform for world model based autonomous driving</h3><div class="text-sm text-gray-600 mb-2"><span>D Gao<!-- -->, </span><span>S Cai<!-- -->, </span><span>H Zhou<!-- -->, </span><span>H Wang<!-- -->, </span><span>I Soltani<!-- -->, </span><span>J Zhang</span></div><div class="flex gap-4 text-sm"><span class="text-gray-600">IEEE Internet of Things Journal</span><span class="text-gray-500 capitalize">journal</span><span class="text-gray-600">2024</span></div></div></div></a><a target="_blank" rel="noopener noreferrer" class="group" href="https://arxiv.org/abs/2410.13973"><div class="space-y-6"><div class="aspect-video relative overflow-hidden rounded-lg bg-gray-100"><video class="w-full h-full object-cover" poster="/publications/USV_MarineFormer.mp4" autoPlay="" muted="" loop="" playsInline="" preload="auto"><source src="/publications/USV_MarineFormer.mp4" type="video/mp4"/></video></div><div><h3 class="text-lg font-medium mb-2 group-hover:text-blue-600 transition-colors">MarineFormer: A Transformer-based Navigation Policy Model for Collision Avoidance in Marine Environment</h3><div class="text-sm text-gray-600 mb-2"><span>E Kazemi<!-- -->, </span><span>I Soltani</span></div><div class="flex gap-4 text-sm"><span class="text-gray-600">arXiv e-prints</span><span class="text-gray-500 capitalize">preprint</span><span class="text-gray-600">2024</span></div></div></div></a><a target="_blank" rel="noopener noreferrer" class="group" href="https://arxiv.org/abs/2409.07914"><div class="space-y-6"><div class="aspect-video relative overflow-hidden rounded-lg bg-gray-100"><video class="w-full h-full object-cover" poster="/publications/InteractDemo.mp4" autoPlay="" muted="" loop="" playsInline="" preload="auto"><source src="/publications/InteractDemo.mp4" type="video/mp4"/></video></div><div><h3 class="text-lg font-medium mb-2 group-hover:text-blue-600 transition-colors">InterACT: Inter-dependency Aware Action Chunking with Hierarchical Attention Transformers for Bimanual Manipulation</h3><div class="text-sm text-gray-600 mb-2"><span>A Lee<!-- -->, </span><span>I Chuang<!-- -->, </span><span>LY Chen<!-- -->, </span><span>I Soltani</span></div><div class="flex gap-4 text-sm"><span class="text-gray-600">Conference on Robot Learning (CoRL)</span><span class="text-gray-500 capitalize">conference</span><span class="text-gray-600">2024</span></div></div></div></a><a target="_blank" rel="noopener noreferrer" class="group" href="https://www.sciencedirect.com/science/article/pii/S266691452400006X"><div class="space-y-6"><div class="aspect-video relative overflow-hidden rounded-lg bg-gray-100"><img alt="Performance of Automated Machine Learning in Predicting Outcomes of Pneumatic Retinopexy" loading="lazy" decoding="async" data-nimg="fill" class="object-cover" style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;color:transparent" src="/publications/retino.png"/></div><div><h3 class="text-lg font-medium mb-2 group-hover:text-blue-600 transition-colors">Performance of Automated Machine Learning in Predicting Outcomes of Pneumatic Retinopexy</h3><div class="text-sm text-gray-600 mb-2"><span>A Nisanova<!-- -->, </span><span>A Yavary<!-- -->, </span><span>J Deaner<!-- -->, </span><span>FS Ali<!-- -->, </span><span>P Gogte<!-- -->, </span><span>R Kaplan<!-- -->, </span><span>KC Chen</span></div><div class="flex gap-4 text-sm"><span class="text-gray-600">Ophthalmology Science 4 (5), 100470</span><span class="text-gray-500 capitalize">journal</span><span class="text-gray-600">2024</span></div></div></div></a><a target="_blank" rel="noopener noreferrer" class="group" href="https://www.sciencedirect.com/science/article/pii/S2590198224001994"><div class="space-y-6"><div class="aspect-video relative overflow-hidden rounded-lg bg-gray-100"><video class="w-full h-full object-cover" poster="/publications/AV_Fully_Compliant_Lane_Change_Compliance_Ablation_movie.mp4" autoPlay="" muted="" loop="" playsInline="" preload="auto"><source src="/publications/AV_Fully_Compliant_Lane_Change_Compliance_Ablation_movie.mp4" type="video/mp4"/></video></div><div><h3 class="text-lg font-medium mb-2 group-hover:text-blue-600 transition-colors">Assessing the impact of driver compliance on traffic flow and safety in work zones amid varied mixed autonomy scenarios</h3><div class="text-sm text-gray-600 mb-2"><span>IS Ehsan Kazemi</span></div><div class="flex gap-4 text-sm"><span class="text-gray-600">Transportation Research Interdisciplinary Perspectives 27</span><span class="text-gray-500 capitalize">journal</span><span class="text-gray-600">2024</span></div></div></div></a><a target="_blank" rel="noopener noreferrer" class="group" href="https://ieeexplore.ieee.org/abstract/document/10433735"><div class="space-y-6"><div class="aspect-video relative overflow-hidden rounded-lg bg-gray-100"><video class="w-full h-full object-cover" autoPlay="" muted="" loop="" playsInline="" preload="auto"><source src="/publications/demo_autodrive.mp4" type="video/mp4"/></video></div><div><h3 class="text-lg font-medium mb-2 group-hover:text-blue-600 transition-colors">Hierarchical end-to-end autonomous navigation through few-shot waypoint detection</h3><div class="text-sm text-gray-600 mb-2"><span>A Ghafourian<!-- -->, </span><span>Z CuiZhu<!-- -->, </span><span>D Shi<!-- -->, </span><span>I Chuang<!-- -->, </span><span>F Charette<!-- -->, </span><span>R Sachdeva</span></div><div class="flex gap-4 text-sm"><span class="text-gray-600">IEEE Robotics and Automation Letters 9 (4), 3211-3218</span><span class="text-gray-500 capitalize">journal</span><span class="text-gray-600">2024</span></div></div></div></a><a target="_blank" rel="noopener noreferrer" class="group" href="https://www.mdpi.com/1424-8220/23/13/6107"><div class="space-y-6"><div class="aspect-video relative overflow-hidden rounded-lg bg-gray-100"><video class="w-full h-full object-cover" poster="/publications/deepbayesian.mp4" autoPlay="" muted="" loop="" playsInline="" preload="auto"><source src="/publications/deepbayesian.mp4" type="video/mp4"/></video></div><div><h3 class="text-lg font-medium mb-2 group-hover:text-blue-600 transition-colors">Deep Bayesian-Assisted Keypoint Detection for Pose Estimation in Assembly Automation</h3><div class="text-sm text-gray-600 mb-2"><span>IS Debo Shi<!-- -->, </span><span>Alireza Rahimpour<!-- -->, </span><span>Amin Ghafourian<!-- -->, </span><span>Mohammad Mahdi Naddaf</span></div><div class="flex gap-4 text-sm"><span class="text-gray-600">Sensors 23 (13)</span><span class="text-gray-500 capitalize">journal</span><span class="text-gray-600">2023</span></div></div></div></a><a target="_blank" rel="noopener noreferrer" class="group" href="https://arxiv.org/abs/2306.08865"><div class="space-y-6"><div class="aspect-video relative overflow-hidden rounded-lg bg-gray-100"><video class="w-full h-full object-cover" poster="/publications/videoplayback.mp4" autoPlay="" muted="" loop="" playsInline="" preload="auto"><source src="/publications/videoplayback.mp4" type="video/mp4"/></video></div><div><h3 class="text-lg font-medium mb-2 group-hover:text-blue-600 transition-colors">One-Shot Learning of Visual Path Navigation for Autonomous Vehicles</h3><div class="text-sm text-gray-600 mb-2"><span>Z CuiZhu<!-- -->, </span><span>F Charette<!-- -->, </span><span>A Ghafourian<!-- -->, </span><span>D Shi<!-- -->, </span><span>M Cui<!-- -->, </span><span>A Krishnamachar</span></div><div class="flex gap-4 text-sm"><span class="text-gray-600">Workshop on Machine Learning for Autonomous Driving (ML4AD)</span><span class="text-gray-500 capitalize">conference</span><span class="text-gray-600">2023</span></div></div></div></a><a target="_blank" rel="noopener noreferrer" class="group" href="https://openaccess.thecvf.com/content/CVPR2023W/VAND/html/Yang_Anomaly_Detection_With_Domain_Adaptation_CVPRW_2023_paper.html"><div class="space-y-6"><div class="aspect-video relative overflow-hidden rounded-lg bg-gray-100"><img alt="Anomaly detection with domain adaptation" loading="lazy" decoding="async" data-nimg="fill" class="object-cover" style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;color:transparent" src="/publications/Anomalydetection.png"/></div><div><h3 class="text-lg font-medium mb-2 group-hover:text-blue-600 transition-colors">Anomaly detection with domain adaptation</h3><div class="text-sm text-gray-600 mb-2"><span>Z Yang<!-- -->, </span><span>I Soltani<!-- -->, </span><span>E Darve</span></div><div class="flex gap-4 text-sm"><span class="text-gray-600">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</span><span class="text-gray-500 capitalize">conference</span><span class="text-gray-600">2023</span></div></div></div></a><a target="_blank" rel="noopener noreferrer" class="group" href="https://ieeexplore.ieee.org/abstract/document/9664442"><div class="space-y-6"><div class="aspect-video relative overflow-hidden rounded-lg bg-gray-100"><img alt="Memory-augmented generative adversarial networks for anomaly detection" loading="lazy" decoding="async" data-nimg="fill" class="object-cover" style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;color:transparent" src="/publications/Memory-augmented.png"/></div><div><h3 class="text-lg font-medium mb-2 group-hover:text-blue-600 transition-colors">Memory-augmented generative adversarial networks for anomaly detection</h3><div class="text-sm text-gray-600 mb-2"><span>Z Yang<!-- -->, </span><span>T Zhang<!-- -->, </span><span>IS Bozchalooi<!-- -->, </span><span>E Darve</span></div><div class="flex gap-4 text-sm"><span class="text-gray-600">IEEE Transactions on Neural Networks and Learning Systems 33 (6), 2324-2334</span><span class="text-gray-500 capitalize">journal</span><span class="text-gray-600">2021</span></div></div></div></a><a target="_blank" rel="noopener noreferrer" class="group" href="https://ebooks.iospress.nl/volumearticle/55067"><div class="space-y-6"><div class="aspect-video relative overflow-hidden rounded-lg bg-gray-100"><video class="w-full h-full object-cover" poster="/publications/regularized.png" autoPlay="" muted="" loop="" playsInline="" preload="auto"><source src="/publications/regularized.png" type="video/mp4"/></video></div><div><h3 class="text-lg font-medium mb-2 group-hover:text-blue-600 transition-colors">Regularized cycle consistent generative adversarial network for anomaly detection</h3><div class="text-sm text-gray-600 mb-2"><span>Z Yang<!-- -->, </span><span>I Soltani Bozchalooi<!-- -->, </span><span>E Darve</span></div><div class="flex gap-4 text-sm"><span class="text-gray-600">ECAI 2020, 1618-1625</span><span class="text-gray-500 capitalize">conference</span><span class="text-gray-600">2020</span></div></div></div></a><a target="_blank" rel="noopener noreferrer" class="group" href="https://pubs.aip.org/avs/jvb/article/35/6/06G101/591631"><div class="space-y-6"><div class="aspect-video relative overflow-hidden rounded-lg bg-gray-100"><video class="w-full h-full object-cover" poster="/publications/jvb.jpg" autoPlay="" muted="" loop="" playsInline="" preload="auto"><source src="/publications/jvb.jpg" type="video/mp4"/></video></div><div><h3 class="text-lg font-medium mb-2 group-hover:text-blue-600 transition-colors">Active scanning probes: A versatile toolkit for fast imaging and emerging nanofabrication</h3><div class="text-sm text-gray-600 mb-2"><span>IW Rangelow<!-- -->, </span><span>T Ivanov<!-- -->, </span><span>A Ahmad<!-- -->, </span><span>M Kaestner<!-- -->, </span><span>C Lenk<!-- -->, </span><span>IS Bozchalooi<!-- -->, </span><span>F Xia<!-- -->, </span><span>K Youcef-Toumi<!-- -->, </span><span>M Holz<!-- -->, </span><span>A Reum</span></div><div class="flex gap-4 text-sm"><span class="text-gray-600">Journal of Vacuum Science &amp; Technology B 35 (6), 06G101</span><span class="text-gray-500 capitalize">journal</span><span class="text-gray-600">2017</span></div></div></div></a><a target="_blank" rel="noopener noreferrer" class="group" href="https://ieeexplore.ieee.org/abstract/document/7963591"><div class="space-y-6"><div class="aspect-video relative overflow-hidden rounded-lg bg-gray-100"><img alt="Induced vibration contact detection for minimizing cantilever tip-sample interaction forces in jumping mode atomic force microscopy" loading="lazy" decoding="async" data-nimg="fill" class="object-cover" style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;color:transparent" src="/publications/Induced-vibration.png"/></div><div><h3 class="text-lg font-medium mb-2 group-hover:text-blue-600 transition-colors">Induced vibration contact detection for minimizing cantilever tip-sample interaction forces in jumping mode atomic force microscopy</h3><div class="text-sm text-gray-600 mb-2"><span>F Xia<!-- -->, </span><span>I Soltani Bozchalooi<!-- -->, </span><span>K Youcef-Toumi</span></div><div class="flex gap-4 text-sm"><span class="text-gray-600">2017 American Control Conference (ACC)</span><span class="text-gray-500 capitalize">conference</span><span class="text-gray-600">2017</span></div></div></div></a><a target="_blank" rel="noopener noreferrer" class="group" href="https://www.sciencedirect.com/science/article/pii/S0304399115300528"><div class="space-y-6"><div class="aspect-video relative overflow-hidden rounded-lg bg-gray-100"><video class="w-full h-full object-cover" poster="/publications/designandcontrol.mp4" autoPlay="" muted="" loop="" playsInline="" preload="auto"><source src="/publications/designandcontrol.mp4" type="video/mp4"/></video></div><div><h3 class="text-lg font-medium mb-2 group-hover:text-blue-600 transition-colors">Design and control of multi-actuated atomic force microscope for large-range and high-speed imaging</h3><div class="text-sm text-gray-600 mb-2"><span>I Soltani Bozchalooi<!-- -->, </span><span>A Careaga Houck<!-- -->, </span><span>JM AlGhamdi<!-- -->, </span><span>K Youcef-Toumi</span></div><div class="flex gap-4 text-sm"><span class="text-gray-600">Ultramicroscopy 160, 213-224</span><span class="text-gray-500 capitalize">journal</span><span class="text-gray-600">2016</span></div></div></div></a><a target="_blank" rel="noopener noreferrer" class="group" href="https://ieeexplore.ieee.org/document/7171869"><div class="space-y-6"><div class="aspect-video relative overflow-hidden rounded-lg bg-gray-100"><img alt="A study on the effectiveness of proportional-integral-derivative control in multi-actuated atomic force microscopy" loading="lazy" decoding="async" data-nimg="fill" class="object-cover" style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;color:transparent" src="/publications/Information-guided.png"/></div><div><h3 class="text-lg font-medium mb-2 group-hover:text-blue-600 transition-colors">A study on the effectiveness of proportional-integral-derivative control in multi-actuated atomic force microscopy</h3><div class="text-sm text-gray-600 mb-2"><span>I Soltani Bozchalooi<!-- -->, </span><span>AC Houck<!-- -->, </span><span>K Youcef-Toumi</span></div><div class="flex gap-4 text-sm"><span class="text-gray-600">2015 American Control Conference (ACC)</span><span class="text-gray-500 capitalize">conference</span><span class="text-gray-600">2015</span></div></div></div></a><a target="_blank" rel="noopener noreferrer" class="group" href="https://ieeexplore.ieee.org/abstract/document/7171011"><div class="space-y-6"><div class="aspect-video relative overflow-hidden rounded-lg bg-gray-100"><img alt="Estimator based multi-eigenmode control of cantilevers in multifrequency Atomic Force Microscopy" loading="lazy" decoding="async" data-nimg="fill" class="object-cover" style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;color:transparent" src="/publications/Estimator-based.png"/></div><div><h3 class="text-lg font-medium mb-2 group-hover:text-blue-600 transition-colors">Estimator based multi-eigenmode control of cantilevers in multifrequency Atomic Force Microscopy</h3><div class="text-sm text-gray-600 mb-2"><span>A Schuh<!-- -->, </span><span>I Soltani Bozchalooi<!-- -->, </span><span>IW Rangelow<!-- -->, </span><span>K Youcef-Toumi</span></div><div class="flex gap-4 text-sm"><span class="text-gray-600">American Control Conference (ACC)</span><span class="text-gray-500 capitalize">conference</span><span class="text-gray-600">2015</span></div></div></div></a></div></div></section><section class="px-6 md:px-10 py-24"><div class="max-w-4xl mx-auto"><div class="flex justify-between items-center mb-16"><h2 class="text-sm font-medium uppercase tracking-wider text-gray-500">Recent Events</h2><a class="text-sm font-medium hover:text-gray-600 transition-colors" href="/blog">View all →</a></div><div class="space-y-8"><div class="flex justify-between items-center py-6 border-b border-gray-100"><div><h3 class="text-lg font-medium mb-2">Sensors Journal: Deep Bayesian-Assisted Keypoint Detection</h3><p class="text-sm text-gray-500">August 21, 2023</p></div><div class="inline-flex items-center rounded-md border font-semibold transition-colors focus:outline-none focus:ring-2 focus:ring-ring focus:ring-offset-2 border-transparent bg-secondary text-secondary-foreground hover:bg-secondary/80 text-xs px-3 py-1">Publication</div></div><div class="flex justify-between items-center py-6 border-b border-gray-100"><div><h3 class="text-lg font-medium mb-2">CVPR 2023</h3><p class="text-sm text-gray-500">April 03, 2023</p></div><div class="inline-flex items-center rounded-md border font-semibold transition-colors focus:outline-none focus:ring-2 focus:ring-ring focus:ring-offset-2 border-transparent bg-secondary text-secondary-foreground hover:bg-secondary/80 text-xs px-3 py-1">Conference</div></div><div class="flex justify-between items-center py-6 border-b border-gray-100"><div><h3 class="text-lg font-medium mb-2">NeurIPS 2022</h3><p class="text-sm text-gray-500">November 13, 2022</p></div><div class="inline-flex items-center rounded-md border font-semibold transition-colors focus:outline-none focus:ring-2 focus:ring-ring focus:ring-offset-2 border-transparent bg-secondary text-secondary-foreground hover:bg-secondary/80 text-xs px-3 py-1">Conference</div></div><div class="flex justify-between items-center py-6 border-b border-gray-100"><div><h3 class="text-lg font-medium mb-2">LARA Seminar Series</h3><p class="text-sm text-gray-500">February 23, 2022</p></div><div class="inline-flex items-center rounded-md border font-semibold transition-colors focus:outline-none focus:ring-2 focus:ring-ring focus:ring-offset-2 border-transparent bg-secondary text-secondary-foreground hover:bg-secondary/80 text-xs px-3 py-1">Event</div></div></div></div></section><section class="px-6 md:px-10 pt-12"><div class="max-w-3xl mx-auto"><div class="relative rounded-2xl p-12 bg-gray-50 md:p-10 overflow-hidden text-center"><div class="relative space-y-8"><div class="space-y-4"><h2 class="text-3xl font-medium">Join Our Research Community</h2><p class="text-gray-600 text-lg mx-auto max-w-xl">Were looking for talented researchers, industry partners, and students to collaborate on cutting-edge projects.</p></div><div class="flex flex-col sm:flex-row items-center justify-center gap-6"><a class="group inline-flex items-center justify-center text-sm bg-black text-white px-8 py-3 rounded-full hover:bg-gray-900 transition-all" href="/contact"><span>Contact Us</span></a><a class="inline-flex items-center justify-center text-sm text-gray-600 hover:text-gray-900 transition-colors" href="/faq">Read FAQ</a></div></div></div></div><footer class="mt-24 border-t border-gray-100 w-full"><div class="py-12"><div class="flex flex-col space-y-8"><div class="flex flex-col md:flex-row md:justify-between gap-4 ml-4 text-[13px]"><div class="space-y-1 text-gray-600"><p>University of California, Davis</p><p>One Shields Avenue, Davis, CA 95616</p></div><div class="flex items-center gap-6 text-gray-400"><a href="/contact" class="hover:text-gray-900 transition-colors">Contact</a><a href="/faq" class="hover:text-gray-900 transition-colors">FAQ</a><span>© <!-- -->2025</span></div></div><div class="flex flex-col md:flex-row md:justify-between items-start md:items-center gap-2 text-[11px] ml-4 text-gray-400"><span>Last updated: <!-- -->December 12, 2025</span><div class="flex items-center gap-1"><span>Built by</span><a href="https://mohnishgopi.com" target="_blank" rel="noopener noreferrer" class="text-gray-600 hover:text-gray-900 transition-colors">Mohnish Gopi</a></div></div></div></div></footer></section></main></div></main></div></div><script src="/_next/static/chunks/webpack-b61c5b23e818b40e.js" async=""></script><script>(self.__next_f=self.__next_f||[]).push([0]);self.__next_f.push([2,null])</script><script>self.__next_f.push([1,"1:HL[\"/_next/static/media/e11418ac562b8ac1-s.p.woff2\",\"font\",{\"crossOrigin\":\"\",\"type\":\"font/woff2\"}]\n2:HL[\"/_next/static/css/ae2e9ad0d33ab6d9.css\",\"style\"]\n"])</script><script>self.__next_f.push([1,"3:I[5751,[],\"\"]\n5:I[6513,[],\"ClientPageRoot\"]\n6:I[4062,[\"173\",\"static/chunks/173-8181ae1d7cdec007.js\",\"231\",\"static/chunks/231-75217c32967b80fc.js\",\"521\",\"static/chunks/521-4c58f4a3393261e6.js\",\"474\",\"static/chunks/474-576ff5c0c60b07ac.js\",\"931\",\"static/chunks/app/page-aea081ca658e47e3.js\"],\"default\"]\n7:I[4858,[\"699\",\"static/chunks/8e1d74a4-561e044f358a70a9.js\",\"173\",\"static/chunks/173-8181ae1d7cdec007.js\",\"231\",\"static/chunks/231-75217c32967b80fc.js\",\"521\",\"static/chunks/521-4c58f4a3393261e6.js\",\"443\",\"static/chunks/443-ccb272207b22f4bd.js\",\"185\",\"static/chunks/app/layout-e9f41310a2e3199e.js\"],\"ThemeProvider\"]\n8:I[9736,[\"699\",\"static/chunks/8e1d74a4-561e044f358a70a9.js\",\"173\",\"static/chunks/173-8181ae1d7cdec007.js\",\"231\",\"static/chunks/231-75217c32967b80fc.js\",\"521\",\"static/chunks/521-4c58f4a3393261e6.js\",\"443\",\"static/chunks/443-ccb272207b22f4bd.js\",\"185\",\"static/chunks/app/layout-e9f41310a2e3199e.js\"],\"TooltipProvider\"]\n9:I[1807,[\"699\",\"static/chunks/8e1d74a4-561e044f358a70a9.js\",\"173\",\"static/chunks/173-8181ae1d7cdec007.js\",\"231\",\"static/chunks/231-75217c32967b80fc.js\",\"521\",\"static/chunks/521-4c58f4a3393261e6.js\",\"443\",\"static/chunks/443-ccb272207b22f4bd.js\",\"185\",\"static/chunks/app/layout-e9f41310a2e3199e.js\"],\"default\"]\na:I[9275,[],\"\"]\nb:I[1343,[],\"\"]\nc:I[231,[\"231\",\"static/chunks/231-75217c32967b80fc.js\",\"160\",\"static/chunks/app/not-found-687fa6ca5be28a47.js\"],\"\"]\ne:I[6130,[],\"\"]\nf:[]\n"])</script><script>self.__next_f.push([1,"0:[[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/css/ae2e9ad0d33ab6d9.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\"}]],[\"$\",\"$L3\",null,{\"buildId\":\"qNKZcHuKywxflDe1xmVy5\",\"assetPrefix\":\"\",\"initialCanonicalUrl\":\"/\",\"initialTree\":[\"\",{\"children\":[\"__PAGE__\",{}]},\"$undefined\",\"$undefined\",true],\"initialSeedData\":[\"\",{\"children\":[\"__PAGE__\",{},[[\"$L4\",[\"$\",\"$L5\",null,{\"props\":{\"params\":{},\"searchParams\":{}},\"Component\":\"$6\"}]],null],null]},[[\"$\",\"html\",null,{\"lang\":\"en\",\"suppressHydrationWarning\":true,\"className\":\"__className_3a0388\",\"children\":[\"$\",\"body\",null,{\"className\":\"antialiased bg-gray-50\",\"children\":[\"$\",\"$L7\",null,{\"attribute\":\"class\",\"defaultTheme\":\"light\",\"children\":[\"$\",\"$L8\",null,{\"delayDuration\":0,\"children\":[\"$\",\"div\",null,{\"className\":\"min-h-screen p-3\",\"children\":[\"$\",\"div\",null,{\"className\":\"fixed inset-3 flex rounded-xl bg-white shadow-sm\",\"children\":[[\"$\",\"$L9\",null,{}],[\"$\",\"main\",null,{\"className\":\"flex-1 overflow-auto\",\"children\":[\"$\",\"div\",null,{\"className\":\"w-full\",\"children\":[\"$\",\"$La\",null,{\"parallelRouterKey\":\"children\",\"segmentPath\":[\"children\"],\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$Lb\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":[\"$\",\"main\",null,{\"className\":\"flex flex-col min-h-screen\",\"children\":[\"$\",\"div\",null,{\"className\":\"flex-grow flex items-center justify-center px-6 md:px-10\",\"children\":[\"$\",\"div\",null,{\"className\":\"max-w-md mx-auto text-center\",\"children\":[[\"$\",\"h1\",null,{\"className\":\"text-4xl font-medium mb-4\",\"children\":\"404\"}],[\"$\",\"h2\",null,{\"className\":\"text-xl mb-4\",\"children\":\"Page Not Found\"}],[\"$\",\"p\",null,{\"className\":\"text-gray-600 mb-8\",\"children\":\"The page you're looking for doesn't exist or has been moved.\"}],[\"$\",\"$Lc\",null,{\"href\":\"/\",\"className\":\"inline-flex items-center text-sm bg-black text-white px-6 py-3 rounded-full hover:bg-gray-800 transition-colors\",\"children\":\"Return Home\"}]]}]}]}],\"notFoundStyles\":[],\"styles\":null}]}]}]]}]}]}]}]}]}],null],null],\"couldBeIntercepted\":false,\"initialHead\":[false,\"$Ld\"],\"globalErrorComponent\":\"$e\",\"missingSlots\":\"$Wf\"}]]\n"])</script><script>self.__next_f.push([1,"d:[[\"$\",\"meta\",\"0\",{\"name\":\"viewport\",\"content\":\"width=device-width, initial-scale=1\"}],[\"$\",\"meta\",\"1\",{\"charSet\":\"utf-8\"}],[\"$\",\"title\",\"2\",{\"children\":\"LARA\"}],[\"$\",\"meta\",\"3\",{\"name\":\"description\",\"content\":\"The Laboratory for AI, Robotics, and Automation (LARA) at UC Davis is at the forefront of research in artificial intelligence and robotics, focusing on the development of autonomous systems that can learn, adapt, and operate in complex environments. Our interdisciplinary research spans across cutting-edge technologies, including machine learning, reinforcement learning, and robotic systems, with applications in automation, autonomous vehicles, and human-robot interaction.\"}],[\"$\",\"meta\",\"4\",{\"name\":\"robots\",\"content\":\"index, follow\"}],[\"$\",\"meta\",\"5\",{\"name\":\"googlebot\",\"content\":\"index, follow, max-video-preview:-1, max-image-preview:large, max-snippet:-1\"}],[\"$\",\"meta\",\"6\",{\"property\":\"og:title\",\"content\":\"LARA\"}],[\"$\",\"meta\",\"7\",{\"property\":\"og:description\",\"content\":\"The Laboratory for AI, Robotics, and Automation (LARA) at UC Davis is at the forefront of research in artificial intelligence and robotics, focusing on the development of autonomous systems that can learn, adapt, and operate in complex environments. Our interdisciplinary research spans across cutting-edge technologies, including machine learning, reinforcement learning, and robotic systems, with applications in automation, autonomous vehicles, and human-robot interaction.\"}],[\"$\",\"meta\",\"8\",{\"property\":\"og:url\",\"content\":\"https://dillion.io\"}],[\"$\",\"meta\",\"9\",{\"property\":\"og:site_name\",\"content\":\"LARA\"}],[\"$\",\"meta\",\"10\",{\"property\":\"og:locale\",\"content\":\"en_US\"}],[\"$\",\"meta\",\"11\",{\"property\":\"og:type\",\"content\":\"website\"}],[\"$\",\"meta\",\"12\",{\"name\":\"twitter:card\",\"content\":\"summary_large_image\"}],[\"$\",\"meta\",\"13\",{\"name\":\"twitter:title\",\"content\":\"LARA\"}],[\"$\",\"meta\",\"14\",{\"name\":\"twitter:description\",\"content\":\"The Laboratory for AI, Robotics, and Automation (LARA) at UC Davis is at the forefront of research in artificial intelligence and robotics, focusing on the development of autonomous systems that can learn, adapt, and operate in complex environments. Our interdisciplinary research spans across cutting-edge technologies, including machine learning, reinforcement learning, and robotic systems, with applications in automation, autonomous vehicles, and human-robot interaction.\"}],[\"$\",\"link\",\"15\",{\"rel\":\"icon\",\"href\":\"/favicon.ico\",\"type\":\"image/x-icon\",\"sizes\":\"16x16\"}],[\"$\",\"meta\",\"16\",{\"name\":\"next-size-adjust\"}]]\n"])</script><script>self.__next_f.push([1,"4:null\n"])</script></body></html>