<!DOCTYPE html><html lang="en" class="__className_3a0388"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width, initial-scale=1"/><link rel="preload" href="/_next/static/media/e11418ac562b8ac1-s.p.woff2" as="font" crossorigin="" type="font/woff2"/><link rel="stylesheet" href="/_next/static/css/ae2e9ad0d33ab6d9.css" data-precedence="next"/><link rel="preload" as="script" fetchPriority="low" href="/_next/static/chunks/webpack-b61c5b23e818b40e.js"/><script src="/_next/static/chunks/fd9d1056-bb11881ef41582aa.js" async=""></script><script src="/_next/static/chunks/23-d4169bb1b2b13904.js" async=""></script><script src="/_next/static/chunks/main-app-bde93d12e61991d7.js" async=""></script><script src="/_next/static/chunks/8e1d74a4-561e044f358a70a9.js" async=""></script><script src="/_next/static/chunks/173-8181ae1d7cdec007.js" async=""></script><script src="/_next/static/chunks/231-75217c32967b80fc.js" async=""></script><script src="/_next/static/chunks/521-4c58f4a3393261e6.js" async=""></script><script src="/_next/static/chunks/443-ccb272207b22f4bd.js" async=""></script><script src="/_next/static/chunks/app/layout-e9f41310a2e3199e.js" async=""></script><script src="/_next/static/chunks/app/blog/%5Bslug%5D/page-21d121c6a2735f65.js" async=""></script><title>AV-ALOHA: Advancing Active Vision in Robotics - Accepted to ICRA 2025 | LARA</title><meta name="description" content="LARA Lab&#x27;s groundbreaking research on active vision in robotic manipulation has been accepted to ICRA 2025"/><meta name="robots" content="index, follow"/><meta name="googlebot" content="index, follow, max-video-preview:-1, max-image-preview:large, max-snippet:-1"/><meta property="og:title" content="AV-ALOHA: Advancing Active Vision in Robotics - Accepted to ICRA 2025"/><meta property="og:description" content="LARA Lab&#x27;s groundbreaking research on active vision in robotic manipulation has been accepted to ICRA 2025"/><meta property="og:url" content="https://dillion.io/blog/av-aloha-icra-2025"/><meta property="og:image" content="https://dillion.io/h0.jpg"/><meta property="og:type" content="article"/><meta property="article:published_time" content="2025-04-22"/><meta name="twitter:card" content="summary_large_image"/><meta name="twitter:title" content="AV-ALOHA: Advancing Active Vision in Robotics - Accepted to ICRA 2025"/><meta name="twitter:description" content="LARA Lab&#x27;s groundbreaking research on active vision in robotic manipulation has been accepted to ICRA 2025"/><meta name="twitter:image" content="https://dillion.io/h0.jpg"/><link rel="icon" href="/favicon.ico" type="image/x-icon" sizes="16x16"/><meta name="next-size-adjust"/><script src="/_next/static/chunks/polyfills-78c92fac7aa8fdd8.js" noModule=""></script></head><body class="antialiased bg-gray-50"><script>!function(){try{var d=document.documentElement,c=d.classList;c.remove('light','dark');var e=localStorage.getItem('theme');if('system'===e||(!e&&false)){var t='(prefers-color-scheme: dark)',m=window.matchMedia(t);if(m.media!==t||m.matches){d.style.colorScheme = 'dark';c.add('dark')}else{d.style.colorScheme = 'light';c.add('light')}}else if(e){c.add(e|| '')}else{c.add('light')}if(e==='light'||e==='dark'||!e)d.style.colorScheme=e||'light'}catch(e){}}()</script><div class="min-h-screen p-3"><div class="fixed inset-3 flex rounded-xl bg-white shadow-sm"><button class="fixed top-4 right-4 z-50 p-2 rounded-md bg-white shadow-sm sm:hidden"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-menu"><line x1="4" x2="20" y1="12" y2="12"></line><line x1="4" x2="20" y1="6" y2="6"></line><line x1="4" x2="20" y1="18" y2="18"></line></svg></button><aside class="fixed inset-y-0 left-0 z-40 w-64 bg-white transform transition-transform duration-200 ease-in-out sm:relative sm:transform-none sm:w-44 sm:border-r sm:border-gray-100 -translate-x-full sm:translate-x-0"><div class="sticky top-0 flex h-full flex-col justify-between py-2"><div><a class="flex items-center justify-center px-5 pt-8 pb-12" href="/"><img alt="LARA Lab" loading="lazy" width="140" height="140" decoding="async" data-nimg="1" style="color:transparent" src="/LARALOGO.png"/></a><nav class="space-y-1 px-2 mb-8"><a class="flex items-center px-3 py-2 text-[13px] transition-all duration-200 rounded-md hover:scale-105 hover:bg-gray-50 text-gray-500 hover:text-gray-900" href="/">Home</a><a class="flex items-center px-3 py-2 text-[13px] transition-all duration-200 rounded-md hover:scale-105 hover:bg-gray-50 text-gray-500 hover:text-gray-900" href="/about">About</a><a class="flex items-center px-3 py-2 text-[13px] transition-all duration-200 rounded-md hover:scale-105 hover:bg-gray-50 text-gray-500 hover:text-gray-900" href="/publications">Publications</a><a class="flex items-center px-3 py-2 text-[13px] transition-all duration-200 rounded-md hover:scale-105 hover:bg-gray-50 text-gray-500 hover:text-gray-900" href="/patents">Patents</a><a class="flex items-center px-3 py-2 text-[13px] transition-all duration-200 rounded-md hover:scale-105 hover:bg-gray-50 text-gray-500 hover:text-gray-900" href="/blog">News</a><a class="flex items-center px-3 py-2 text-[13px] transition-all duration-200 rounded-md hover:scale-105 hover:bg-gray-50 text-gray-500 hover:text-gray-900" href="/people">People</a><a class="flex items-center px-3 py-2 text-[13px] transition-all duration-200 rounded-md hover:scale-105 hover:bg-gray-50 text-gray-500 hover:text-gray-900" href="/faq">FAQ</a><a class="flex items-center px-3 py-2 text-[13px] transition-all duration-200 rounded-md hover:scale-105 hover:bg-gray-50 text-gray-500 hover:text-gray-900" href="/contact">Contact</a></nav></div><div class="space-y-8 px-5 pb-6"><a href="https://drive.google.com/file/d/1PuFn_1D4t18b7H_9VPRS3ilNOY-dlGjO/view" target="_blank" rel="noopener noreferrer" class="group flex items-center gap-2 text-[11px] text-gray-400 hover:text-gray-900 transition-colors"><span>View brand guidelines</span><svg xmlns="http://www.w3.org/2000/svg" width="10" height="10" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-arrow-right transition-transform group-hover:translate-x-0.5"><path d="M5 12h14"></path><path d="m12 5 7 7-7 7"></path></svg></a><div class="flex gap-4"><a href="https://linkedin.com" target="_blank" rel="noopener noreferrer" class="text-gray-400 hover:text-gray-900 transition-all hover:scale-110 duration-200" aria-label="LinkedIn"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 448 512" height="15" width="15" xmlns="http://www.w3.org/2000/svg"><path d="M416 32H31.9C14.3 32 0 46.5 0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6 0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3zM135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3 0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2 0 38.5 17.3 38.5 38.5 0 21.3-17.2 38.5-38.5 38.5zm282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6 0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2 0 79.7 44.3 79.7 101.9V416z"></path></svg></a><a href="https://scholar.google.com" target="_blank" rel="noopener noreferrer" class="text-gray-400 hover:text-gray-900 transition-all hover:scale-110 duration-200" aria-label="Google Scholar"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 488 512" height="15" width="15" xmlns="http://www.w3.org/2000/svg"><path d="M488 261.8C488 403.3 391.1 504 248 504 110.8 504 0 393.2 0 256S110.8 8 248 8c66.8 0 123 24.5 166.3 64.9l-67.5 64.9C258.5 52.6 94.3 116.6 94.3 256c0 86.5 69.1 156.6 153.7 156.6 98.2 0 135-70.4 140.8-106.9H248v-85.3h236.1c2.3 12.7 3.9 24.9 3.9 41.4z"></path></svg></a><a href="https://github.com" target="_blank" rel="noopener noreferrer" class="text-gray-400 hover:text-gray-900 transition-all hover:scale-110 duration-200" aria-label="GitHub"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 496 512" height="15" width="15" xmlns="http://www.w3.org/2000/svg"><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"></path></svg></a><a href="mailto:isoltani@ucdavis.edu" class="text-gray-400 hover:text-gray-900 transition-all hover:scale-110 duration-200" aria-label="Email"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 512 512" height="15" width="15" xmlns="http://www.w3.org/2000/svg"><path d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"></path></svg></a></div></div></div></aside><main class="flex-1 overflow-auto"><div class="w-full"><section class="max-w-4xl mx-auto px-6"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","headline":"AV-ALOHA: Advancing Active Vision in Robotics - Accepted to ICRA 2025","datePublished":"2025-04-22","dateModified":"2025-04-22","description":"LARA Lab's groundbreaking research on active vision in robotic manipulation has been accepted to ICRA 2025","image":"https://dillion.io/og?title=AV-ALOHA: Advancing Active Vision in Robotics - Accepted to ICRA 2025","url":"https://dillion.io/blog/av-aloha-icra-2025","author":{"@type":"Person","name":"LARA"}}</script><div class="py-6"><a class="inline-flex items-center text-[15px] text-gray-600 hover:text-gray-900" href="/blog"><svg class="mr-2 w-4 h-4" fill="none" viewBox="0 0 24 24" stroke="currentColor"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M10 19l-7-7m0 0l7-7m-7 7h18"></path></svg>Back to all posts</a></div><div class="pt-8 pb-10 max-w-3xl mx-auto text-center"><div class="mb-3 flex items-center justify-center"><span class="text-sm text-gray-500">April 22, 2025 (7mo ago)</span></div><h1 class="text-4xl md:text-5xl font-medium tracking-tight mb-6">AV-ALOHA: Advancing Active Vision in Robotics - Accepted to ICRA 2025</h1><p class="text-[17px] text-gray-600 max-w-2xl mx-auto">LARA Lab&#x27;s groundbreaking research on active vision in robotic manipulation has been accepted to ICRA 2025</p></div><div class="mb-12"><img alt="AV-ALOHA: Advancing Active Vision in Robotics - Accepted to ICRA 2025" loading="lazy" width="1200" height="630" decoding="async" data-nimg="1" class="w-full h-auto rounded-lg" style="color:transparent" src="/h0.jpg"/></div><article class="max-w-3xl mx-auto"><div class="prose prose-gray max-w-none text-[15px]"><h1>AV-ALOHA: Pioneering Active Vision in Robotic Manipulation</h1>
<p>We are thrilled to announce that our research paper "Active Vision Might Be All You Need" has been accepted for presentation at ICRA 2025. This work represents a significant milestone in our ongoing research at LARA Lab, where we're advancing the capabilities of robotic systems through innovative active vision approaches.</p>
<h2>The Challenge of "Simple" Tasks</h2>
<p>What comes naturally to humans often proves surprisingly complex for robots. As Dr. Soltani notes, "Some of the things that we take for granted as humans are so simple that we don't even pay attention to how we achieve them. We have evolved over millions of years to achieve certain capabilities. When you start to think in terms of robotics, you realize that they're quite complex."</p>
<p>Consider a task as straightforward as threading a needle or pouring liquid from one test tube to another. Humans instinctively adjust their viewpoint to get the best angle, a skill that has proven notoriously difficult to program into robots – until now.</p>
<h2>Active Vision: Teaching Robots to See Like Humans</h2>
<p>Our research introduces AV-ALOHA, a groundbreaking system that enables robots to actively control their "point of view." Unlike traditional fixed-camera systems, AV-ALOHA can dynamically adjust its perspective to gather the most relevant information for any given task.</p>
<p>The system comprises three key components:</p>
<ul>
<li>Two robotic arms for manipulation tasks</li>
<li>A dedicated 7-DoF camera arm for active vision</li>
<li>A VR-based control interface for intuitive human demonstration</li>
</ul>
<p>Through this setup, operators can control the manipulation arms while simultaneously adjusting the camera's viewpoint using natural head movements, all while receiving real-time visual feedback through the VR headset.</p>
<h2>From Demonstration to Autonomy</h2>
<p>Our research goes beyond mere teleoperation. Through imitation learning, AV-ALOHA learns not just how to manipulate objects, but also how to position its camera for optimal task performance. We conducted extensive experiments across five simulation tasks and one real-world scenario, each designed to test different aspects of active vision:</p>
<ul>
<li>Peg insertion</li>
<li>Slot insertion</li>
<li>Hook package manipulation</li>
<li>Test tube pouring</li>
<li>Thread needle insertion</li>
<li>Occluded insertion (real-world)</li>
</ul>
<p>The results were compelling: in scenarios where perspective matters most – particularly the thread needle test and occluded insertion – AV-ALOHA significantly outperformed systems with multiple fixed cameras.</p>
<h2>Unexpected Insights</h2>
<p>One fascinating discovery was that in tasks where varying perspectives weren't critical, the active vision system performed just as well as traditional setups with multiple fixed cameras. This suggests that a single, well-positioned camera might be sufficient for many tasks, potentially simplifying future robotic systems.</p>
<h2>Looking Forward: The Next Phase</h2>
<p>Our acceptance to ICRA 2025 marks not an endpoint but a milestone in ongoing research. We're currently exploring several intriguing questions:</p>
<ul>
<li>How do robots balance camera movement with manipulation tasks?</li>
<li>Should perspective adjustment precede or coincide with manipulation?</li>
<li>Can we optimize head movement to mirror human behavior?</li>
</ul>
<p>As Dr. Soltani explains, "We think that in more complex scenarios, most likely, we don't want to keep moving our heads while trying to complete a sensitive task with our hands." This insight is driving our next research phase, where we'll explore penalizing excessive head movement during precise manipulation tasks.</p>
<h2>The Research Team</h2>
<p>This breakthrough represents a collaborative effort between UC Berkeley and UC Davis researchers:</p>
<ul>
<li>Ian Chuang* (UC Berkeley)</li>
<li>Andrew Lee* (UC Davis)</li>
<li>Dechen Gao (UC Davis)</li>
<li>M-Mahdi Naddaf-Sh (UC Davis)</li>
<li>Iman Soltani (UC Davis)</li>
</ul>
<p>*Equal contribution</p>
<h2>Join Us at ICRA 2025</h2>
<p>We look forward to presenting these findings at ICRA 2025, where we'll demonstrate live system operations and discuss future applications in industrial automation, surgical robotics, and human-robot collaboration.</p>
<h2>Learn More</h2>
<p>Explore our research in detail:</p>
<ul>
<li><a href="https://soltanilara.github.io/av-aloha/">Project Website</a></li>
<li><a href="https://arxiv.org/abs/2409.17435">Research Paper</a></li>
<li><a href="https://soltanilara.github.io/av-aloha/">Technical Demonstrations</a></li>
</ul>
<hr>
<p><em>For collaboration inquiries or more information about our research, please contact us. We look forward to engaging with the robotics research community at ICRA 2025.</em></p></div></article><div class="max-w-3xl mx-auto mt-12 pt-8 border-t border-gray-100"><h2 class="text-xl font-medium mb-4">Authors</h2><div class="flex items-center"><div class="w-12 h-12 rounded-full bg-gray-200 mr-4"></div><div><h3 class="text-base font-medium">LARA Lab</h3><p class="text-[15px] text-gray-600">The Laboratory for AI, Robotics, and Automation at UC Davis</p></div></div></div><div class="max-w-3xl mx-auto mt-16 pt-8 border-t border-gray-100"><h2 class="text-xl font-medium mb-6">Related Research</h2><div class="space-y-4"><p class="text-[15px] text-gray-600">Explore more research from our lab</p></div></div><footer class="max-w-3xl mx-auto mt-16 pt-8 pb-16 border-t border-gray-100 text-sm text-gray-600"><p>University of California, Davis</p><p>One Shields Avenue, Davis, CA 95616</p><p class="mt-4">© The Regents of the University of California, Davis campus.</p></footer></section></div></main></div></div><script src="/_next/static/chunks/webpack-b61c5b23e818b40e.js" async=""></script><script>(self.__next_f=self.__next_f||[]).push([0]);self.__next_f.push([2,null])</script><script>self.__next_f.push([1,"1:HL[\"/_next/static/media/e11418ac562b8ac1-s.p.woff2\",\"font\",{\"crossOrigin\":\"\",\"type\":\"font/woff2\"}]\n2:HL[\"/_next/static/css/ae2e9ad0d33ab6d9.css\",\"style\"]\n"])</script><script>self.__next_f.push([1,"3:I[5751,[],\"\"]\n6:I[9275,[],\"\"]\n8:I[1343,[],\"\"]\n9:I[4858,[\"699\",\"static/chunks/8e1d74a4-561e044f358a70a9.js\",\"173\",\"static/chunks/173-8181ae1d7cdec007.js\",\"231\",\"static/chunks/231-75217c32967b80fc.js\",\"521\",\"static/chunks/521-4c58f4a3393261e6.js\",\"443\",\"static/chunks/443-ccb272207b22f4bd.js\",\"185\",\"static/chunks/app/layout-e9f41310a2e3199e.js\"],\"ThemeProvider\"]\na:I[9736,[\"699\",\"static/chunks/8e1d74a4-561e044f358a70a9.js\",\"173\",\"static/chunks/173-8181ae1d7cdec007.js\",\"231\",\"static/chunks/231-75217c32967b80fc.js\",\"521\",\"static/chunks/521-4c58f4a3393261e6.js\",\"443\",\"static/chunks/443-ccb272207b22f4bd.js\",\"185\",\"static/chunks/app/layout-e9f41310a2e3199e.js\"],\"TooltipProvider\"]\nb:I[1807,[\"699\",\"static/chunks/8e1d74a4-561e044f358a70a9.js\",\"173\",\"static/chunks/173-8181ae1d7cdec007.js\",\"231\",\"static/chunks/231-75217c32967b80fc.js\",\"521\",\"static/chunks/521-4c58f4a3393261e6.js\",\"443\",\"static/chunks/443-ccb272207b22f4bd.js\",\"185\",\"static/chunks/app/layout-e9f41310a2e3199e.js\"],\"default\"]\nc:I[231,[\"173\",\"static/chunks/173-8181ae1d7cdec007.js\",\"231\",\"static/chunks/231-75217c32967b80fc.js\",\"308\",\"static/chunks/app/blog/%5Bslug%5D/page-21d121c6a2735f65.js\"],\"\"]\ne:I[6130,[],\"\"]\n7:[\"slug\",\"av-aloha-icra-2025\",\"d\"]\nf:[]\n"])</script><script>self.__next_f.push([1,"0:[[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/css/ae2e9ad0d33ab6d9.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\"}]],[\"$\",\"$L3\",null,{\"buildId\":\"pNEwLN9gc_9YMHEzjq_a0\",\"assetPrefix\":\"\",\"initialCanonicalUrl\":\"/blog/av-aloha-icra-2025\",\"initialTree\":[\"\",{\"children\":[\"blog\",{\"children\":[[\"slug\",\"av-aloha-icra-2025\",\"d\"],{\"children\":[\"__PAGE__?{\\\"slug\\\":\\\"av-aloha-icra-2025\\\"}\",{}]}]}]},\"$undefined\",\"$undefined\",true],\"initialSeedData\":[\"\",{\"children\":[\"blog\",{\"children\":[[\"slug\",\"av-aloha-icra-2025\",\"d\"],{\"children\":[\"__PAGE__\",{},[[\"$L4\",\"$L5\"],null],null]},[\"$\",\"$L6\",null,{\"parallelRouterKey\":\"children\",\"segmentPath\":[\"children\",\"blog\",\"children\",\"$7\",\"children\"],\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L8\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"notFoundStyles\":\"$undefined\",\"styles\":null}],null]},[\"$\",\"$L6\",null,{\"parallelRouterKey\":\"children\",\"segmentPath\":[\"children\",\"blog\",\"children\"],\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L8\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"notFoundStyles\":\"$undefined\",\"styles\":null}],null]},[[\"$\",\"html\",null,{\"lang\":\"en\",\"suppressHydrationWarning\":true,\"className\":\"__className_3a0388\",\"children\":[\"$\",\"body\",null,{\"className\":\"antialiased bg-gray-50\",\"children\":[\"$\",\"$L9\",null,{\"attribute\":\"class\",\"defaultTheme\":\"light\",\"children\":[\"$\",\"$La\",null,{\"delayDuration\":0,\"children\":[\"$\",\"div\",null,{\"className\":\"min-h-screen p-3\",\"children\":[\"$\",\"div\",null,{\"className\":\"fixed inset-3 flex rounded-xl bg-white shadow-sm\",\"children\":[[\"$\",\"$Lb\",null,{}],[\"$\",\"main\",null,{\"className\":\"flex-1 overflow-auto\",\"children\":[\"$\",\"div\",null,{\"className\":\"w-full\",\"children\":[\"$\",\"$L6\",null,{\"parallelRouterKey\":\"children\",\"segmentPath\":[\"children\"],\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L8\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":[\"$\",\"main\",null,{\"className\":\"flex flex-col min-h-screen\",\"children\":[\"$\",\"div\",null,{\"className\":\"flex-grow flex items-center justify-center px-6 md:px-10\",\"children\":[\"$\",\"div\",null,{\"className\":\"max-w-md mx-auto text-center\",\"children\":[[\"$\",\"h1\",null,{\"className\":\"text-4xl font-medium mb-4\",\"children\":\"404\"}],[\"$\",\"h2\",null,{\"className\":\"text-xl mb-4\",\"children\":\"Page Not Found\"}],[\"$\",\"p\",null,{\"className\":\"text-gray-600 mb-8\",\"children\":\"The page you're looking for doesn't exist or has been moved.\"}],[\"$\",\"$Lc\",null,{\"href\":\"/\",\"className\":\"inline-flex items-center text-sm bg-black text-white px-6 py-3 rounded-full hover:bg-gray-800 transition-colors\",\"children\":\"Return Home\"}]]}]}]}],\"notFoundStyles\":[],\"styles\":null}]}]}]]}]}]}]}]}]}],null],null],\"couldBeIntercepted\":false,\"initialHead\":[false,\"$Ld\"],\"globalErrorComponent\":\"$e\",\"missingSlots\":\"$Wf\"}]]\n"])</script><script>self.__next_f.push([1,"10:I[8173,[\"173\",\"static/chunks/173-8181ae1d7cdec007.js\",\"231\",\"static/chunks/231-75217c32967b80fc.js\",\"308\",\"static/chunks/app/blog/%5Bslug%5D/page-21d121c6a2735f65.js\"],\"Image\"]\n11:T1312,"])</script><script>self.__next_f.push([1,"\u003ch1\u003eAV-ALOHA: Pioneering Active Vision in Robotic Manipulation\u003c/h1\u003e\n\u003cp\u003eWe are thrilled to announce that our research paper \"Active Vision Might Be All You Need\" has been accepted for presentation at ICRA 2025. This work represents a significant milestone in our ongoing research at LARA Lab, where we're advancing the capabilities of robotic systems through innovative active vision approaches.\u003c/p\u003e\n\u003ch2\u003eThe Challenge of \"Simple\" Tasks\u003c/h2\u003e\n\u003cp\u003eWhat comes naturally to humans often proves surprisingly complex for robots. As Dr. Soltani notes, \"Some of the things that we take for granted as humans are so simple that we don't even pay attention to how we achieve them. We have evolved over millions of years to achieve certain capabilities. When you start to think in terms of robotics, you realize that they're quite complex.\"\u003c/p\u003e\n\u003cp\u003eConsider a task as straightforward as threading a needle or pouring liquid from one test tube to another. Humans instinctively adjust their viewpoint to get the best angle, a skill that has proven notoriously difficult to program into robots – until now.\u003c/p\u003e\n\u003ch2\u003eActive Vision: Teaching Robots to See Like Humans\u003c/h2\u003e\n\u003cp\u003eOur research introduces AV-ALOHA, a groundbreaking system that enables robots to actively control their \"point of view.\" Unlike traditional fixed-camera systems, AV-ALOHA can dynamically adjust its perspective to gather the most relevant information for any given task.\u003c/p\u003e\n\u003cp\u003eThe system comprises three key components:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eTwo robotic arms for manipulation tasks\u003c/li\u003e\n\u003cli\u003eA dedicated 7-DoF camera arm for active vision\u003c/li\u003e\n\u003cli\u003eA VR-based control interface for intuitive human demonstration\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThrough this setup, operators can control the manipulation arms while simultaneously adjusting the camera's viewpoint using natural head movements, all while receiving real-time visual feedback through the VR headset.\u003c/p\u003e\n\u003ch2\u003eFrom Demonstration to Autonomy\u003c/h2\u003e\n\u003cp\u003eOur research goes beyond mere teleoperation. Through imitation learning, AV-ALOHA learns not just how to manipulate objects, but also how to position its camera for optimal task performance. We conducted extensive experiments across five simulation tasks and one real-world scenario, each designed to test different aspects of active vision:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003ePeg insertion\u003c/li\u003e\n\u003cli\u003eSlot insertion\u003c/li\u003e\n\u003cli\u003eHook package manipulation\u003c/li\u003e\n\u003cli\u003eTest tube pouring\u003c/li\u003e\n\u003cli\u003eThread needle insertion\u003c/li\u003e\n\u003cli\u003eOccluded insertion (real-world)\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThe results were compelling: in scenarios where perspective matters most – particularly the thread needle test and occluded insertion – AV-ALOHA significantly outperformed systems with multiple fixed cameras.\u003c/p\u003e\n\u003ch2\u003eUnexpected Insights\u003c/h2\u003e\n\u003cp\u003eOne fascinating discovery was that in tasks where varying perspectives weren't critical, the active vision system performed just as well as traditional setups with multiple fixed cameras. This suggests that a single, well-positioned camera might be sufficient for many tasks, potentially simplifying future robotic systems.\u003c/p\u003e\n\u003ch2\u003eLooking Forward: The Next Phase\u003c/h2\u003e\n\u003cp\u003eOur acceptance to ICRA 2025 marks not an endpoint but a milestone in ongoing research. We're currently exploring several intriguing questions:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eHow do robots balance camera movement with manipulation tasks?\u003c/li\u003e\n\u003cli\u003eShould perspective adjustment precede or coincide with manipulation?\u003c/li\u003e\n\u003cli\u003eCan we optimize head movement to mirror human behavior?\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eAs Dr. Soltani explains, \"We think that in more complex scenarios, most likely, we don't want to keep moving our heads while trying to complete a sensitive task with our hands.\" This insight is driving our next research phase, where we'll explore penalizing excessive head movement during precise manipulation tasks.\u003c/p\u003e\n\u003ch2\u003eThe Research Team\u003c/h2\u003e\n\u003cp\u003eThis breakthrough represents a collaborative effort between UC Berkeley and UC Davis researchers:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eIan Chuang* (UC Berkeley)\u003c/li\u003e\n\u003cli\u003eAndrew Lee* (UC Davis)\u003c/li\u003e\n\u003cli\u003eDechen Gao (UC Davis)\u003c/li\u003e\n\u003cli\u003eM-Mahdi Naddaf-Sh (UC Davis)\u003c/li\u003e\n\u003cli\u003eIman Soltani (UC Davis)\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e*Equal contribution\u003c/p\u003e\n\u003ch2\u003eJoin Us at ICRA 2025\u003c/h2\u003e\n\u003cp\u003eWe look forward to presenting these findings at ICRA 2025, where we'll demonstrate live system operations and discuss future applications in industrial automation, surgical robotics, and human-robot collaboration.\u003c/p\u003e\n\u003ch2\u003eLearn More\u003c/h2\u003e\n\u003cp\u003eExplore our research in detail:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://soltanilara.github.io/av-aloha/\"\u003eProject Website\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://arxiv.org/abs/2409.17435\"\u003eResearch Paper\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://soltanilara.github.io/av-aloha/\"\u003eTechnical Demonstrations\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003cp\u003e\u003cem\u003eFor collaboration inquiries or more information about our research, please contact us. We look forward to engaging with the robotics research community at ICRA 2025.\u003c/em\u003e\u003c/p\u003e"])</script><script>self.__next_f.push([1,"5:[\"$\",\"section\",null,{\"className\":\"max-w-4xl mx-auto px-6\",\"children\":[[\"$\",\"script\",null,{\"type\":\"application/ld+json\",\"suppressHydrationWarning\":true,\"dangerouslySetInnerHTML\":{\"__html\":\"{\\\"@context\\\":\\\"https://schema.org\\\",\\\"@type\\\":\\\"BlogPosting\\\",\\\"headline\\\":\\\"AV-ALOHA: Advancing Active Vision in Robotics - Accepted to ICRA 2025\\\",\\\"datePublished\\\":\\\"2025-04-22\\\",\\\"dateModified\\\":\\\"2025-04-22\\\",\\\"description\\\":\\\"LARA Lab's groundbreaking research on active vision in robotic manipulation has been accepted to ICRA 2025\\\",\\\"image\\\":\\\"https://dillion.io/og?title=AV-ALOHA: Advancing Active Vision in Robotics - Accepted to ICRA 2025\\\",\\\"url\\\":\\\"https://dillion.io/blog/av-aloha-icra-2025\\\",\\\"author\\\":{\\\"@type\\\":\\\"Person\\\",\\\"name\\\":\\\"LARA\\\"}}\"}}],[\"$\",\"div\",null,{\"className\":\"py-6\",\"children\":[\"$\",\"$Lc\",null,{\"href\":\"/blog\",\"className\":\"inline-flex items-center text-[15px] text-gray-600 hover:text-gray-900\",\"children\":[[\"$\",\"svg\",null,{\"className\":\"mr-2 w-4 h-4\",\"fill\":\"none\",\"viewBox\":\"0 0 24 24\",\"stroke\":\"currentColor\",\"children\":[\"$\",\"path\",null,{\"strokeLinecap\":\"round\",\"strokeLinejoin\":\"round\",\"strokeWidth\":2,\"d\":\"M10 19l-7-7m0 0l7-7m-7 7h18\"}]}],\"Back to all posts\"]}]}],[\"$\",\"div\",null,{\"className\":\"pt-8 pb-10 max-w-3xl mx-auto text-center\",\"children\":[[\"$\",\"div\",null,{\"className\":\"mb-3 flex items-center justify-center\",\"children\":[[\"$\",\"span\",null,{\"className\":\"text-sm text-gray-500\",\"children\":\"April 22, 2025 (7mo ago)\"}],\"$undefined\"]}],[\"$\",\"h1\",null,{\"className\":\"text-4xl md:text-5xl font-medium tracking-tight mb-6\",\"children\":\"AV-ALOHA: Advancing Active Vision in Robotics - Accepted to ICRA 2025\"}],[\"$\",\"p\",null,{\"className\":\"text-[17px] text-gray-600 max-w-2xl mx-auto\",\"children\":\"LARA Lab's groundbreaking research on active vision in robotic manipulation has been accepted to ICRA 2025\"}]]}],[\"$\",\"div\",null,{\"className\":\"mb-12\",\"children\":[\"$\",\"$L10\",null,{\"src\":\"/h0.jpg\",\"alt\":\"AV-ALOHA: Advancing Active Vision in Robotics - Accepted to ICRA 2025\",\"width\":1200,\"height\":630,\"className\":\"w-full h-auto rounded-lg\"}]}],false,[\"$\",\"article\",null,{\"className\":\"max-w-3xl mx-auto\",\"children\":[\"$\",\"div\",null,{\"className\":\"prose prose-gray max-w-none text-[15px]\",\"dangerouslySetInnerHTML\":{\"__html\":\"$11\"}}]}],[\"$\",\"div\",null,{\"className\":\"max-w-3xl mx-auto mt-12 pt-8 border-t border-gray-100\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"text-xl font-medium mb-4\",\"children\":\"Authors\"}],[\"$\",\"div\",null,{\"className\":\"flex items-center\",\"children\":[[\"$\",\"div\",null,{\"className\":\"w-12 h-12 rounded-full bg-gray-200 mr-4\"}],[\"$\",\"div\",null,{\"children\":[[\"$\",\"h3\",null,{\"className\":\"text-base font-medium\",\"children\":\"LARA Lab\"}],[\"$\",\"p\",null,{\"className\":\"text-[15px] text-gray-600\",\"children\":\"The Laboratory for AI, Robotics, and Automation at UC Davis\"}]]}]]}]]}],[\"$\",\"div\",null,{\"className\":\"max-w-3xl mx-auto mt-16 pt-8 border-t border-gray-100\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"text-xl font-medium mb-6\",\"children\":\"Related Research\"}],[\"$\",\"div\",null,{\"className\":\"space-y-4\",\"children\":[\"$\",\"p\",null,{\"className\":\"text-[15px] text-gray-600\",\"children\":\"Explore more research from our lab\"}]}]]}],[\"$\",\"footer\",null,{\"className\":\"max-w-3xl mx-auto mt-16 pt-8 pb-16 border-t border-gray-100 text-sm text-gray-600\",\"children\":[[\"$\",\"p\",null,{\"children\":\"University of California, Davis\"}],[\"$\",\"p\",null,{\"children\":\"One Shields Avenue, Davis, CA 95616\"}],[\"$\",\"p\",null,{\"className\":\"mt-4\",\"children\":\"© The Regents of the University of California, Davis campus.\"}]]}]]}]\n"])</script><script>self.__next_f.push([1,"d:[[\"$\",\"meta\",\"0\",{\"name\":\"viewport\",\"content\":\"width=device-width, initial-scale=1\"}],[\"$\",\"meta\",\"1\",{\"charSet\":\"utf-8\"}],[\"$\",\"title\",\"2\",{\"children\":\"AV-ALOHA: Advancing Active Vision in Robotics - Accepted to ICRA 2025 | LARA\"}],[\"$\",\"meta\",\"3\",{\"name\":\"description\",\"content\":\"LARA Lab's groundbreaking research on active vision in robotic manipulation has been accepted to ICRA 2025\"}],[\"$\",\"meta\",\"4\",{\"name\":\"robots\",\"content\":\"index, follow\"}],[\"$\",\"meta\",\"5\",{\"name\":\"googlebot\",\"content\":\"index, follow, max-video-preview:-1, max-image-preview:large, max-snippet:-1\"}],[\"$\",\"meta\",\"6\",{\"property\":\"og:title\",\"content\":\"AV-ALOHA: Advancing Active Vision in Robotics - Accepted to ICRA 2025\"}],[\"$\",\"meta\",\"7\",{\"property\":\"og:description\",\"content\":\"LARA Lab's groundbreaking research on active vision in robotic manipulation has been accepted to ICRA 2025\"}],[\"$\",\"meta\",\"8\",{\"property\":\"og:url\",\"content\":\"https://dillion.io/blog/av-aloha-icra-2025\"}],[\"$\",\"meta\",\"9\",{\"property\":\"og:image\",\"content\":\"https://dillion.io/h0.jpg\"}],[\"$\",\"meta\",\"10\",{\"property\":\"og:type\",\"content\":\"article\"}],[\"$\",\"meta\",\"11\",{\"property\":\"article:published_time\",\"content\":\"2025-04-22\"}],[\"$\",\"meta\",\"12\",{\"name\":\"twitter:card\",\"content\":\"summary_large_image\"}],[\"$\",\"meta\",\"13\",{\"name\":\"twitter:title\",\"content\":\"AV-ALOHA: Advancing Active Vision in Robotics - Accepted to ICRA 2025\"}],[\"$\",\"meta\",\"14\",{\"name\":\"twitter:description\",\"content\":\"LARA Lab's groundbreaking research on active vision in robotic manipulation has been accepted to ICRA 2025\"}],[\"$\",\"meta\",\"15\",{\"name\":\"twitter:image\",\"content\":\"https://dillion.io/h0.jpg\"}],[\"$\",\"link\",\"16\",{\"rel\":\"icon\",\"href\":\"/favicon.ico\",\"type\":\"image/x-icon\",\"sizes\":\"16x16\"}],[\"$\",\"meta\",\"17\",{\"name\":\"next-size-adjust\"}]]\n4:null\n"])</script></body></html>